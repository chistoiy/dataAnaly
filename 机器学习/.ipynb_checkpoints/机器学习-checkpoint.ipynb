{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一部分\n",
    "***\n",
    "\n",
    "# 机器学习\n",
    "数据文件csv\n",
    "+ 数据大，使用mysql存储有性能瓶颈，读取速度\n",
    "\n",
    "+ pandas：读取工具，\n",
    "+ numpy：释放python的GIL\n",
    "\n",
    "+ 数据集结构：特征值+目标值（有些时候可以没有）；DataFrame格式\n",
    "\n",
    "+ 特征工程：将原始数据转换为更好的代表预测模型的潜在问题的特征的过程，从而提高对位置数据的预测准确性\n",
    "\n",
    "+ 使用Scikit-learn 库进行数据处理;文档完善\n",
    "\n",
    "  `` pip install sklearn``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据的特征化\n",
    "### 1.1 字典，数组特征值化;\n",
    "sklearn.feature_extraction.DictVectorizer\n",
    "+ DictVectorizer.fit_transform(x)，传入字典或包含字典的迭代器返回sparse矩阵，\n",
    "+ .inverse_transform(x)，传入array数组或sparse矩阵，返回转换之前的数据格式\n",
    "+ .get_feature_names()返回类别名称，即如果是字典时，是字典的key，或是数组的列名\n",
    "+ .transform(x)按原本标准转换\n",
    "\n",
    "### 1.2  对文本数据进行特征值化\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "+ CountVectorizer()返回词频矩阵\n",
    "+ .fit_transform(x)，传入文本或包含文本字符串的迭代器返回sparse矩阵，\n",
    "+ .inverse_transform(x)，传入array数组或sparse矩阵，返回转换之前的数据格式\n",
    "+ .get_feature_names()返回类别名称， "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city=上海', 'city=北京', 'city=深圳', 'temperature']\n",
      "[{'city=北京': 1.0, 'temperature': 100.0}, {'city=上海': 1.0, 'temperature': 80.0}, {'city=深圳': 1.0, 'temperature': 99.0}]\n",
      "[[  0.   1.   0. 100.]\n",
      " [  1.   0.   0.  80.]\n",
      " [  0.   0.   1.  99.]]\n"
     ]
    }
   ],
   "source": [
    "#字典，数组特征值化;\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def dictvec(dic):\n",
    "    dict = DictVectorizer(sparse=False)#参数sparse默认是true，设置为false则返回一个矩阵\n",
    "    data = dict.fit_transform(dic)\n",
    "    print(dict.get_feature_names())\n",
    "    print(dict.inverse_transform(data))\n",
    "    return data\n",
    "\n",
    "dic = [{\"city\":'北京',\"temperature\":100},{\"city\":'上海',\"temperature\":80},{\"city\":'深圳',\"temperature\":99}]\n",
    "da = dictvec(dic)\n",
    "print(da)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 输出矩阵的列信息：['city=上海', 'city=北京', 'city=深圳', 'temperature']\n",
    "data的内容：打印矩阵，需要设置dict = DictVectorizer(sparse=False)#参数sparse默认是true，设置为false则返回一个矩阵\n",
    "[[  0.   1.   0. 100.]\n",
    " [  1.   0.   0.  80.]\n",
    " [  0.   0.   1.  99.]] one-hot编码：将有类别的（字符串形式出现的特征值）特征值名进行编码，也就是针对原字典的key，建立bool列，此处对city建立三个列，分别为上海，北京，深圳，然后dic字典内第一个为北京，因此这个（0，1）位置为1，第二个位上海，因此（1，0）为1，对于temperature特征则直接赋值\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'like', 'love', 'many', 'people', 'the', 'world']\n",
      "=================\n",
      "[array(['people', 'many', 'love'], dtype='<U6'), array(['world', 'the', 'like', 'just', 'love'], dtype='<U6')]\n",
      "[[0 0 1 2 1 0 0]\n",
      " [1 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#特征文本特征抽取\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dictvec(st):\n",
    "    cv = CountVectorizer()\n",
    "    data = cv.fit_transform(st)\n",
    "    print(cv.get_feature_names())\n",
    "    print('=================')\n",
    "    print(cv.inverse_transform(data))\n",
    "    return data\n",
    "\n",
    "st =[\"i love many many people\",\"just like i love the world\"]\n",
    "da = dictvec(st)\n",
    "print(da.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['just', 'like', 'love', 'many', 'people', 'the', 'world'],矩阵列名，统计所有文章中所有的词，重复的只看做一次,单个英文字母不处理统计\n",
    "输出的特征化矩阵：输出矩阵形式需要：使用toarray()方法\n",
    "[[0 0 1 2 1 0 0] \n",
    " [1 1 1 0 0 1 1]]  one-hot,建立，按文章的顺序，来统计文章中每一个词汇在对应列下出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['不予', '人生', '岁月', '清风', '漫漫', '长进']\n",
      "=================\n",
      "[array(['长进', '漫漫', '人生'], dtype='<U2'), array(['不予', '岁月', '清风'], dtype='<U2')]\n",
      "[[0 1 0 0 1 1]\n",
      " [1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#中文处理时，默认不支持，只会根据符号（表单符号，空格）来划分，对单个汉字同样不处理\n",
    "#可以使用jieba分词来操作，将文章分词后使用一个空格连接成一个str\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dictvec(st):\n",
    "    cv = CountVectorizer()\n",
    "    data = cv.fit_transform(st)\n",
    "    print(cv.get_feature_names())\n",
    "    print('=================')\n",
    "    print(cv.inverse_transform(data))\n",
    "    return data\n",
    "import jieba\n",
    "st =[\"人生漫漫，好不。长进\",\"清风岁月不予心\"]\n",
    "st = [' '.join(list(jieba.cut(i))) for i in st]\n",
    "da = dictvec(st)\n",
    "print(da.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 中性词的处理：\n",
    "因为不同文本里的中性词，即如代词，介词，连词一类对文章本身不具有特别的情感色彩的界定，但这些词又占有一定比例（不同文本内这些词比例一样时，不能说两篇文本是相似的，因此需要其他解决方法）\n",
    "文本抽取方式：\n",
    "1. tf-idf\n",
    "  tf term frequency 词频   \n",
    "  idf inverse document frequency 逆文档频率  ㏒(总文档数量/该词出现的文档数量)，即一个词在多个文档中出现过；  \n",
    "  重要性 tf*idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['不予', '人生', '因此', '岁月', '清风', '漫漫', '长进']\n",
      "=================\n",
      "[array(['人生', '漫漫', '长进'], dtype='<U2'), array(['清风', '岁月', '不予', '因此'], dtype='<U2')]\n",
      "[[0.         0.57735027 0.         0.         0.         0.57735027\n",
      "  0.57735027]\n",
      " [0.5        0.         0.5        0.5        0.5        0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#中性词的处理\n",
    "#sklearn.feature_extraction.text.TfidVectorizer\n",
    " \n",
    "#TfidVectorizer()\n",
    "#     返回词的权重矩阵\n",
    "#fit_transform(x)，传入文本或包含文本字符串的迭代器返回sparse矩阵，\n",
    "#.inverse_transform(x)，传入array数组或sparse矩阵，返回转换之前的数据格式\n",
    "#.get_feature_names()返回单词列表，\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dictvec(st):\n",
    "    cv = TfidfVectorizer()\n",
    "    data = cv.fit_transform(st)\n",
    "    print(cv.get_feature_names())\n",
    "    print('=================')\n",
    "    print(cv.inverse_transform(data))\n",
    "   \n",
    "    return data\n",
    "import jieba\n",
    "st =[\"人生漫漫，好不。长进\",\"清风岁月不予心的我，因此\"]\n",
    "st = [' '.join(list(jieba.cut(i))) for i in st]\n",
    "da = dictvec(st)\n",
    "print(da.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征预处理：\n",
    "### 2.1 数值类型的数据：\n",
    "+ 归一化\n",
    "    - 通过对原始数据进行变换把数据映射到（默认为【0，1】）之间\n",
    "    - 公式：\n",
    "      ``X` = （x-min） /(max-min)   X`` = X`*(mx-mi)+mi``  \n",
    "    \n",
    "      作用与每一列，max为一列最大值，min为列最小值，X``为最终结果（按列对应位置赋值），mx，mi为指定区间值，默认mx=1，mi=0\n",
    "\n",
    "    - 作用：数据集里特征有多个，且针对个别列值特别大，如A列均在5000左右，B列在10以内，则数据相关不一致，经过归一化后，均落点在（0，1）内方便计算，使得某一个特征对最终结果不会造成更大影响\n",
    "    - 异常点：对最大值最小值影响太大，在公式中对结果影响过大（新增某一个点）\n",
    "      - 只适合传统精确小数据场景\n",
    "\n",
    "+ 标准化：\n",
    "    - 通过对原始数据进行变换到均值在0，方差为1的范围内\n",
    "    - 公式  \n",
    "        X` = (x-mean)/σ\n",
    "        - 作用与每一列，\n",
    "        - mean为平均值，σ为标准差，var为方差\n",
    "        - var = (x1=mean)²+(x2+mean)²+...  /n\n",
    "        - σ = √var    \n",
    "        \n",
    "    - 异常点的影响因为有数据量的存在，对平均值影响较小，方差改变也较小\n",
    "    - 适合样本多的情况下\n",
    "\n",
    "+ 缺失值\n",
    "    - 删除：每列或行数据缺失达到一定比例，放弃该列/行\n",
    "    - 填补：根据行，列，均值，中位数填补\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.        ]\n",
      " [0.         1.         1.         0.83333333]\n",
      " [0.5        0.5        0.6        1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "k = mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,13,46]])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22474487 -1.22474487 -1.29777137 -1.3970014 ]\n",
      " [-1.22474487  1.22474487  1.13554995  0.50800051]\n",
      " [ 0.          0.          0.16222142  0.88900089]]\n",
      "[75.          3.         12.66666667 43.66666667]\n",
      "[150.           0.66666667   4.22222222   6.88888889]\n"
     ]
    }
   ],
   "source": [
    "#标准化\n",
    "#StandardScaler()\n",
    "#     \n",
    "#fit_transform(x)，传入numpy array格式数据返回sparse矩阵，\n",
    "#.inverse_transform(x)，传入array数组或sparse矩阵，返回转换之前的数据格式\n",
    "#.get_feature_names()返回单词列表，\n",
    "#.var_原始数据每列特征的方差\n",
    "#.mean_每列特征均值\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mm = StandardScaler()\n",
    "k = mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,13,46]])\n",
    "print(k)\n",
    "print(mm.mean_)\n",
    "print(mm.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90.   3.5 10.  40. ]\n",
      " [60.   4.  11.5 45. ]\n",
      " [75.   3.  13.  46. ]]\n",
      "[[90, nan, 10, 40], [60, 4, nan, 45], [75, 3, 13, 46]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#缺失值\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "import numpy as np\n",
    "mm = Imputer(missing_values=\"NaN\",strategy='mean',axis=0)\n",
    "li = [[90,np.nan,10,40],[60,4,np.nan,45],[75,3,13,46]]\n",
    "k = mm.fit_transform(li)\n",
    "print(k)\n",
    "print(li )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据降维\n",
    "### 3.1 特征数量减少  \n",
    "方式一，特征选择：sklearn.feature_selection\n",
    "  >  从所有特征中选择部分特征作为训练集特征，对于特征值在选择前后可以改变   \n",
    "    冗余：个别特征是不需要的，浪费计算性能   \n",
    "    噪声：部分特征对预测结果有影响   \n",
    "    主要方法：  \n",
    "    Filter 过滤 针对方差的过滤  \n",
    "    Embedded嵌入  \n",
    "    Wrapper包裹  \n",
    "    其他方法：\n",
    "    神经网络  \n",
    "    \n",
    "方式二，主成分分析：sklearn.decompostion\n",
    "   > PCA:分析简化数据集技术  \n",
    "    数据维数压缩。尽可能降低原数据维数，损失少量信息  \n",
    "    可以削减回归分析或具类分析中特征数量  \n",
    "    高纬度数据容易出现的问题：  \n",
    "        特征间可能存在相关性      \n",
    "    如一个二维的数据，表现上为一个二维数轴上的点，降维则可以将所有点投影到数轴的y=x线上，这样点的个数不变，但是维度降低了。\n",
    "    公式：Y = PX \n",
    "    矩阵运算得到:  \n",
    "    ``P= （1/√2  1/√2\n",
    "        -1/√2 1/√2）\n",
    "    Y = (1/√2  1/√2)    (-1 -1 0 2 0 \n",
    "                           -2 0  0 1 0)   =(-3/√2  -1/√2 0 3/√2  -1/√2)``\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90.  10.  40. ]\n",
      " [60.  11.5 45. ]\n",
      " [75.  13.  46. ]]\n"
     ]
    }
   ],
   "source": [
    "#Filter\n",
    "\n",
    "#VarianceThreshold(threshold=0.0)\n",
    "#.fit_transform(X)，传入numpy。array格式数据，返回训练集差异低于threshold的特征将被删除，默认保留所有非零方差特征，删除所有样本中具有相同值的特征\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold(threshold=1.0)\n",
    "v = vt.fit_transform([[90,4,10,40],\n",
    "                     [60,4,11.5,45],\n",
    "                     [75,4,13,46]])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 主成分分析PCA\n",
    "* PCA(n_components=None)数据分解到较低维数空间 n_components: 小数 0-1 之间，保留的特征值比例，整数：减少到的特征值数量\n",
    "* PCA.fit_transform(X)  传入numpy.array个数数据，返回指定维度的array  \n",
    "\n",
    "`` from sklearn.decomposition import PCA\n",
    "    p = PCA(n_components=0.9)\n",
    "data = p.fit_transform([[90,4,10,40],\n",
    "                     [60,4,11.5,45],\n",
    "                     [75,4,13,46]])\n",
    "print(data)  ``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = pd.read_csv('./market_csv/order_products__prior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('./market_csv/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv('./market_csv/orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisles = pd.read_csv('./market_csv/aisles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mg = pd.merge(prior,products,on=['product_id','product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mg = pd.merge(_mg,orders,on=['order_id','order_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = pd.merge(_mg,aisles,on=['aisle_id','aisle_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  product_id  add_to_cart_order  reordered        product_name  \\\n",
      "0         2       33120                  1          1  Organic Egg Whites   \n",
      "1        26       33120                  5          0  Organic Egg Whites   \n",
      "2       120       33120                 13          0  Organic Egg Whites   \n",
      "3       327       33120                  5          1  Organic Egg Whites   \n",
      "4       390       33120                 28          1  Organic Egg Whites   \n",
      "5       537       33120                  2          1  Organic Egg Whites   \n",
      "6       582       33120                  7          1  Organic Egg Whites   \n",
      "7       608       33120                  5          1  Organic Egg Whites   \n",
      "8       623       33120                  1          1  Organic Egg Whites   \n",
      "9       689       33120                  4          1  Organic Egg Whites   \n",
      "\n",
      "   aisle_id  department_id  user_id eval_set  order_number  order_dow  \\\n",
      "0        86             16   202279    prior             3          5   \n",
      "1        86             16   153404    prior             2          0   \n",
      "2        86             16    23750    prior            11          6   \n",
      "3        86             16    58707    prior            21          6   \n",
      "4        86             16   166654    prior            48          0   \n",
      "5        86             16   180135    prior            15          2   \n",
      "6        86             16   193223    prior             6          2   \n",
      "7        86             16    91030    prior            11          3   \n",
      "8        86             16    37804    prior            63          3   \n",
      "9        86             16   108932    prior            16          1   \n",
      "\n",
      "   order_hour_of_day  days_since_prior_order aisle  \n",
      "0                  9                     8.0  eggs  \n",
      "1                 16                     7.0  eggs  \n",
      "2                  8                    10.0  eggs  \n",
      "3                  9                     8.0  eggs  \n",
      "4                 12                     9.0  eggs  \n",
      "5                  8                     3.0  eggs  \n",
      "6                 19                    10.0  eggs  \n",
      "7                 21                    12.0  eggs  \n",
      "8                 12                     3.0  eggs  \n",
      "9                 13                     3.0  eggs  \n"
     ]
    }
   ],
   "source": [
    "print(me.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.crosstab(me['user_id'],me['aisle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cross.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.9)#保留百分之九十特征\n",
    "data = pca.fit_transform(cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.42156587e+01  2.42942720e+00 -2.46636975e+00 ...  6.86800336e-01\n",
      "   1.69439402e+00 -2.34323022e+00]\n",
      " [ 6.46320806e+00  3.67511165e+01  8.38255336e+00 ...  4.12121252e+00\n",
      "   2.44689740e+00 -4.28348478e+00]\n",
      " [-7.99030162e+00  2.40438257e+00 -1.10300641e+01 ...  1.77534453e+00\n",
      "  -4.44194030e-01  7.86665571e-01]\n",
      " ...\n",
      " [ 8.61143331e+00  7.70129866e+00  7.95240226e+00 ... -2.74252456e+00\n",
      "   1.07112531e+00 -6.31925661e-02]\n",
      " [ 8.40862199e+01  2.04187340e+01  8.05410372e+00 ...  7.27554259e-01\n",
      "   3.51339470e+00 -1.79079914e+01]\n",
      " [-1.39534562e+01  6.64621821e+00 -5.23030367e+00 ...  8.25329076e-01\n",
      "   1.38230701e+00 -2.41942061e+00]]\n",
      "(206209, 27)\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    i   j   k   l   m\n",
      "a   1   2   3   4   5\n",
      "b   6   7   8   9  10\n",
      "c  11  12  13  14  15\n",
      "d  16  17  18  19  20\n",
      "e  21  22  23  24  25\n",
      "    i   j     k   l   m\n",
      "a   1   2  jiao   4   5\n",
      "b   1   7  jiao   9  10\n",
      "c  11  12    ll  14  15\n",
      "d  16  17    18  19  20\n",
      "e  21  22  jiao  24  25\n",
      "=======\n",
      "a     1\n",
      "b     1\n",
      "c    11\n",
      "d    16\n",
      "e    21\n",
      "Name: i, dtype: int32\n",
      "=======\n",
      "k   18  jiao  ll\n",
      "i               \n",
      "1    0     2   0\n",
      "11   0     0   1\n",
      "16   1     0   0\n",
      "21   0     1   0\n",
      "=========\n",
      "i\n",
      "1     0\n",
      "11    0\n",
      "16    1\n",
      "21    0\n",
      "Name: 18, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#crosstab，交叉分组，相当于这样一个流程，从一个列表中抽取两列，A列的元素取set（）当作index,B列的元素取set()为colums，\n",
    "    #设置行，列索引时，过程如此:\n",
    "    #    设置一个全0数组，行列索引分别为A，B，然后回原数组按行为一组取A，B的值，即为新数组的一个行列的loc取值元组，如果该位置为0，则赋值1，否则累加即可。\n",
    "    #    即新数组保存了原AB两列同行间的关系，以及每一类的相同行（类别）的个数\n",
    "a = pd.DataFrame(np.arange(1,26).reshape(5,5),index=list('abcde'),columns=list('ijklm'))\n",
    "print(a)\n",
    "\n",
    "a.loc['a','k']='jiao'\n",
    "a.loc['b','k']='jiao'\n",
    "a.loc['e','k']='jiao'\n",
    "a.loc['c','k']='ll'\n",
    "a.loc['b','i']=1\n",
    "print(a)\n",
    "print('=======')\n",
    "print(a['i'])\n",
    "print('=======')\n",
    "b = pd.crosstab(a['i'],a['k'])\n",
    "print(b)\n",
    "print('=========')\n",
    "print(b[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分\n",
    "***\n",
    "# 机器学习基础\n",
    "   * 核心：算法，\n",
    "   * 基础：数据和计算\n",
    "\n",
    "## 1.1 数据类型：\n",
    "  + 离散型数据：计数数据，全都是整数，不能再被细分，也无法进一步提高精确度\n",
    "  + 连续性数据：变量可以在一定范围内取任一数，通常是非整数的，长度，时间，质量值，可被细分\n",
    "  \n",
    "## 1.2 机器学习算法分类\n",
    "  * 监督学习（预测）:特征值+目标值   \n",
    "      * 分类（离散型数据） k-近邻算法，贝叶斯分类，决策树与随机森林，逻辑回归，神经网络   \n",
    "      * 回归（连续型数据） 线性回归，岭回归   \n",
    "      * 标注 隐马尔科夫模型   \n",
    "  * 无监督学习：特征值  \n",
    "      * 聚类 k-means  \n",
    "      \n",
    "## 1.3 业务流程\n",
    "   * 建立模型：根据数据类型划分应用种类\n",
    "       >  \n",
    "          原属数据明确问题内容\n",
    "          数据基本处理：pd处理数据（缺省值，合并表）\n",
    "          特征工程（特征进行处理）\n",
    "          合适的算法预测\n",
    "          模型评估\n",
    "\n",
    "## 1.4 数据集划分\n",
    "   + 训练数据\n",
    "   + 测试数据，评估模型是否有效\n",
    "   \n",
    "## 1.5 加载数据集\n",
    " sklearn.datasets\n",
    "+ .load_*()#小规模数据集，数据包含在datasets里\n",
    "+ .fetch_*(data_home=None)#获取大规模数据集，需要网络下载，需要传入数据集下载的目录，默认为 ~/scikit_learn_data/\n",
    "返回的数据类型都是datasets.base.Bunch(字典格式),一般包含的参数值有\n",
    "   + data:特征数据数组，是[n_samples*n_features]的二位numpy.ndarray数组\n",
    "   + target：标签数组，是n_samples的一维numpy ndarray数组\n",
    "   + DESCR：数据描述\n",
    "   + feature_names：特征名，新闻数据，手写数字、回归数据集没有\n",
    "   + target_names：标签名  \n",
    "   \n",
    "  ### 1.5.1 用于分类的大数据集\n",
    "   + sklearn.datasets.fetch_20newsgroups(data_home=None,subset='train')\n",
    "        - subset:'train','all','test'可选，选择加载（没有则自动下载）的数据集为：‘训练’，‘全部’，‘测试’\n",
    "   + datasets.clear_data_home(data_home=None)\n",
    "        清除目录下的数据\n",
    "        \n",
    "  ### 1.5.2 sklearn回归数据集\n",
    "   + sklearn.datasets.load_boston()加载返回波士顿房价数据集\n",
    "   + sklearn.datasets.load_diabetes() 加载返回糖尿病数据集\n",
    "   \n",
    "## 1.6 分割数据集\n",
    "   sklearn.model_selection.train_test_split(*arrays,**options)  \n",
    "   配置参数：  \n",
    "   \n",
    "   + x:数据集特征值 \n",
    "   + y:数据集标签值\n",
    "   + test_size 测试集大小，一般为float，0-1之间\n",
    "   + random_state 随机数种子，同一个种子的采样结果一样\n",
    "   + return 训练集特征值，测试集特征值，训练标签，测试标签（默认随机取）\n",
    "    \n",
    "## 1.7转换器\n",
    "   fit_transform()：具有输入数据与转换功能\n",
    "   +     fit():只输入数据\n",
    "   +    transform()：进行数据转换\n",
    "   + transform()转换的值，只根据最近的fit()传入值来执行的\n",
    "   \n",
    "## 1.8估计器\n",
    "实现算法的API\n",
    " ### 1.8.1 分类估计器\n",
    "   + sklearn.neighbors k近邻\n",
    "   + sklearn.naive_bayes 贝叶斯\n",
    "   + sklearn.liner_model.LogisticRegression 逻辑回归\n",
    "   + sklearn.tree 决策树与随机森林  \n",
    "   \n",
    "### 1.8.2 回归估计器\n",
    "   + sklearn.liner_model.LinearRegression 线性回归\n",
    "   + sklearn.liner_model.Ridge 岭回归\n",
    "    \n",
    "### 1.8.3 估计器工作流程： \n",
    " >   ↓输入训练集  \n",
    "        fit(x_train,y_train)  \n",
    "        y_predict = predict(x_test)  \n",
    "        score(x_test,y_test) 评估结果  \n",
    "    ↑输入测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "li = load_iris()\n",
    "print(li.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "a = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(a.data)\n",
    "#print(a.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "bo = load_boston()\n",
    "#print(bo.target)\n",
    "#print(bo.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.4 3.9 1.7 0.4]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.6 2.5 3.9 1.1]]\n"
     ]
    }
   ],
   "source": [
    "#分割数据集\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "li = load_iris()\n",
    "data = train_test_split(li.data,li.target,test_size=0.25)\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三部分\n",
    "***\n",
    "# 1. K-近邻算法\n",
    "+ 通过距离公式求得距离最近的特征样本\n",
    "    - 所谓K意思为，先根据一个有特征值和目标值的数据集，计算这些数据的距离与分类的关系；然后使用测试数据时，测试集内的一条数据的特征值计算距离后，取K个最相近的测试集数据，然后找出这K个测试集数据中大多数属于的类别，这个类别就是这条测试数据归属的类别。\n",
    "+ api: sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "\t- n_neighbors:int，可选，默认为5，k_neighbors查询默认使用的邻居数\n",
    "\t- algorithm：{auto,ball_tree,kd_tree,brute}计算最近邻算法\n",
    "\t- ball_tree 使用BallTree\n",
    "    - kd_tree 使用BDTree\n",
    "    - auto 尝试根据传递的fit方法的值来决定最适合的算法\n",
    "    \n",
    "+ 两个样本的距离算法：\n",
    "    欧式距离 如  \n",
    "     > `` a(a1,a2,a3),b(b1,b2,b3)   \n",
    "     distance = √((a1-b1)²+(a2-b2)²+(a3-b3)²)``  \n",
    "    \n",
    "+ 数据需要做标准化\n",
    "+ 使用：   \n",
    "\t> knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')  \n",
    "    knn.fit(x_train,y_train) 录入训练集的特征值和标签  \n",
    "    y_predict = knn.predict(x_test) 对测试集的特征集进行计算，返回预测的标签值  \n",
    "    score(x_test,y_test) 评估结果 输入测试特征值与实际的标签值，根据上一步得到的预测值进行对比得到精确度    \n",
    "    \n",
    "+ 关于k值 \n",
    "    - K值设置小时，异常点影响大 \n",
    "    - 设置大时，受k值数量（类别）波动\n",
    "+ 性能   \n",
    "    - 简单，便于理解，易于实现，无需估计参数，无需训练\n",
    "+ 懒惰算法，对测试样本分类时计算量大，内存开销大\n",
    "    - 必须指定k，k选择不当分类精度不能保证使用场景\n",
    "    \n",
    "+ 案例：\n",
    "  - 对[facebook客户位置坐标推荐住店](https://www.kaggle.com/c/facebook-v-predicting-check-ins/data)  \n",
    "  - 数据集内容：\n",
    "      - row_id: 入住编号（事件基本无关）id of the check-in event\n",
    "      - x y: 用户的坐标coordinates\n",
    "      - accuracy: 坐标精确度，location accuracy \n",
    "      - time: 时间戳timestamp\n",
    "      - place_id:目标值， id of the business, this is the target you are predicting\n",
    "      \n",
    "  - 流程：\n",
    "      - 导入数据\n",
    "          - pandas导入数据集\n",
    "      - 数据处理\n",
    "          - pandas.query\n",
    "          - pandas.to_datetime\n",
    "          - pandas.DatetimeIndex\n",
    "          - 分割 \n",
    "              - from sklearn.model_selection import train_test_split\n",
    "              - x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "      - K近邻\n",
    "          - knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "          - knn.fit(x_train,y_train)#录入数据 \n",
    "          - y_predict = knn.predict(x_test)#得到测试结果\n",
    "          - knn.score(x_test,y_test)准确度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./facebook_csv/train.csv')#,nrows=2000000)\n",
    "\n",
    "#print(data.head(10))\n",
    "#print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "600      600  1.2214  2.7023        17  6683426742    1    18        3\n",
      "957      957  1.1832  2.6891        58  6683426742   10     2        5\n",
      "4345    4345  1.1935  2.6550        11  6889790653    5    15        0\n",
      "4735    4735  1.1452  2.6074        49  6822359752    6    23        1\n",
      "5580    5580  1.0089  2.7287        19  1527921905    9    11        4\n",
      "6090    6090  1.1140  2.6262        11  4000153867    2    16        4\n",
      "6234    6234  1.1449  2.5003        34  3741484405    4    15        6\n",
      "6350    6350  1.0844  2.7436        65  5963693798    1    10        3\n",
      "7468    7468  1.0058  2.5096        66  9076695703    9    15        4\n",
      "8478    8478  1.2015  2.5187        72  3992589015    8    23        3\n"
     ]
    }
   ],
   "source": [
    "# 查找坐标范围\n",
    "data = data.query('x > 1.0 & x < 1.25 &y >2.5 & y<2.75')\n",
    "#print(data)\n",
    "timevalue = pd.to_datetime(data['time'],unit='s')\n",
    "time = pd.DatetimeIndex(timevalue)#将时间序列转换为字典\n",
    "data['day'] = time.day\n",
    "data['hour'] = time.hour\n",
    "data['weekday'] = time.weekday\n",
    "data = data.drop(['time'],axis=1)\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08274231678486997\n"
     ]
    }
   ],
   "source": [
    "num = data.groupby(by='place_id').count()\n",
    "#print(num)\n",
    "#保留三次以上的点\n",
    "tf = num[num.row_id>3].reset_index()#将分组依据重新划分为分组后的列元素\n",
    "#print(tf)\n",
    "data = data[data['place_id'].isin(tf.place_id)]#保留符合条件的地点id\n",
    "#print(data)\n",
    "y = data['place_id']\n",
    "x = data.drop(['place_id'],axis=1)\n",
    "#分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "\n",
    "#录入数据\n",
    "knn.fit(x_train,y_train)\n",
    "#得到测试结果\n",
    "y_predict = knn.predict(x_test)\n",
    "#print(y_predict)\n",
    "\n",
    "#准确度\n",
    "print(knn.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48652482269503544\n"
     ]
    }
   ],
   "source": [
    "# k近邻数据标准化\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./facebook_csv/train.csv')#,nrows=2000000)\n",
    "\n",
    "data = data.query('x > 1.0 & x < 1.25 &y >2.5 & y<2.75')\n",
    "#print(data)\n",
    "timevalue = pd.to_datetime(data['time'],unit='s')\n",
    "time = pd.DatetimeIndex(timevalue)\n",
    "data['day'] = time.day\n",
    "data['hour'] = time.hour\n",
    "data['weekday'] = time.weekday\n",
    "data = data.drop(['time'],axis=1)\n",
    " \n",
    "num = data.groupby(by='place_id').count()\n",
    " \n",
    "tf = num[num.row_id>3].reset_index()#将分组依据重新划分为分组后的列元素\n",
    " \n",
    "data = data[data['place_id'].isin(tf.place_id)]#保留符合条件的地点id\n",
    " \n",
    "y = data['place_id']\n",
    "x = data.drop(['place_id'],axis=1)\n",
    "\n",
    "x = x.drop(['row_id'],axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.fit_transform(x_test)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "\n",
    "#录入数据\n",
    "knn.fit(x_train,y_train)\n",
    "#得到测试结果\n",
    "y_predict = knn.predict(x_test)\n",
    "\n",
    "\n",
    "#准确度\n",
    "print(knn.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# 鸢尾花数据集k近邻预测\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "li = load_iris()\n",
    "print(len(li.target))\n",
    "print(len(li.data))\n",
    "x = li.data\n",
    "y = li.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "\n",
    "#录入数据\n",
    "knn.fit(x_train,y_train)\n",
    "#得到测试结果\n",
    "y_predict = knn.predict(x_test)\n",
    "#print(y_predict)\n",
    "\n",
    "#准确度\n",
    "print(knn.score(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 朴素贝叶斯算法\n",
    "   + 特征独立  \n",
    "    \n",
    "## 2.1 概率基础\n",
    "   + 条件概率\n",
    "       - 事件A在另一事件B已发生条件下发生的概率(A1,A2相互独立)\n",
    "       > P(A|B)    \n",
    "       P(A1，A2|B)=P(A1|B)P(A2|B)\n",
    "         \n",
    "   + 联合概率\n",
    "       - 包含多个条件，且所有条件同时成立的概率\n",
    "       > P(A,B) = P(A)P(B)\n",
    "       \n",
    "## 2.2 朴素贝叶斯-贝叶斯公式\n",
    "   + 文档有多个词（有一定情感的），\n",
    "   >P(C|W) = P(W|C)P(C) / P(W)\n",
    "       - W为给定文档的特征值（频数统计，预测文档提供，）\n",
    "       - C为文档类别  \n",
    "       \n",
    "   >公式可以变换为：P(C|F1,F2,F3,F4...) = P(F1,F2,F3,F4...|C)P(C) / P(F1,F2,F3,F4...)\n",
    "       - C 可以为不同类别\n",
    "       \n",
    "       - 实际计算为：P(C|F1,F2,F3,F4...) = P(F1,F2,F3,F4...|C)P(C)\n",
    "                           =P(F1|C)P(F2|C)P(F3|C)P(F4|C)...P(C)\n",
    "           - P(F1|C) = 这个词在C类别下出现的次数 / 这个词在C类别下所有的特征词的总个数（非类别，而是各特征值对应出现的总次数）\n",
    "              - 这个词在C类别下出现的次数 ，如C为科技类别，那么训练集内，划分到科技类别的文章中这个词共出现的次数\n",
    "              - 分母为，科技类别下，所有的特征词出现的次数的总和，\n",
    "           - P(C) = 训练集中C类别下文章篇数 / 训练集总文章篇数\n",
    "           \n",
    "       - 有些时候计算结果可能是0.，则解决方法：拉普拉斯平滑系数\n",
    "       > P(F1|C) = (Ni+α) / (N+ αm)\n",
    "         - α为指定的系数，一般为1，m为训练文档中统计出的特征词个数（也就是特征词有几个类别，不是这些特征词出现的频数）\n",
    "         - Ni 为这个词在C类别下出现的次数\n",
    "         - N 科技类别下，所有的特征词出现的次数的总和\n",
    "         - 也就是将所有的P(F|C)变成上式的内容\n",
    "         \n",
    "## 2.3 API\n",
    "   + sklearn.naive_bayes.MultinomialNB(alpha = 1.0) \n",
    "        - alpha = 1.0 意即拉普拉斯平滑系数中的α\n",
    "        \n",
    "## 2.4 优缺点\n",
    "   + 发源于古典数学理论，稳定的分类效率\n",
    "   + 对缺失数据不太敏感，算法较简单，常用文本分类\n",
    "   + 分类准确度高，速度快\n",
    "  ***\n",
    "   + 样本属性独立性假设，如果样本属性有关联时效果则不好\n",
    "   \n",
    "## 2.5. 分类模型的精确率与召回率\n",
    "+ 分类模型的评估：\n",
    "    + estimator.score() 一般常见使用的准确率，预测结果正确的百分比\n",
    "    \n",
    "    ### 2.5.1 混淆矩阵\n",
    "    \n",
    "     + 分类任务下，预测结果与正确结果标记之间存在四种不同组合，构成混淆矩阵（多分类）    \n",
    "\n",
    "\n",
    " - |正例 |反例 \n",
    "      :-: | :-: | :-: \n",
    "    正例 | 真正例TP | 伪反例FN |\n",
    "    假例 | 伪反例FP | 真正例TN |\n",
    " \n",
    "      如一个二分类中，只有猫狗两个分类，对于猫这个测试类别有这样的混淆矩阵，列代表预测结果，行代表真实情况 \n",
    "  - |cat  |NO cat \n",
    "       :-: | :-: | :-: \n",
    "    cat |true positive | false negtive |\n",
    "   NO cat | false positive | true negtive |\n",
    "   \n",
    "***      \n",
    "        \n",
    "   + 召回率 = TP/（TP+FN） 得到的是预测对的占真实中对的比率\n",
    "   + 精确率 = TP/（TP+FP） 得到的预测个数中真正对的占的比率\n",
    "   + 稳健率F1-score = 2TP/(2TP+FN+FP) = 2*Precision*Recall /(Precision+Recall)\n",
    "   + API\n",
    "        + sklearn.metrics.classification_report(y_true,y_pred,target_names=None)\n",
    "            - y_true:真实目标值\n",
    "            - y_pred:关于机器预测目标值\n",
    "            - target_names:目标类别名称\n",
    "            - return 每个类别精确率与召回率\n",
    "            \n",
    "            \n",
    "## 2.6 交叉验证与网格搜索（超参数搜索）\n",
    "     \n",
    "   ### 2.6.1交叉验证\n",
    "   \n",
    "   + 为了让被评估的模型更加准确可信\n",
    "   + 如将数据集分为固定的4份（编号1，2，3，4），取1号当作验证集，其他为训练集，得到一个准确率1\n",
    "       - 取2号做验证集，其他为训练集，得到准确率2依次得到准确率3，4\n",
    "       - 求四个准确率率的平均值，该平均值为当前模型的**准确率**\n",
    "       - 其中分成4份，为**4折交叉验证**\n",
    "   + 一般与网格搜索配合使用 \n",
    "   \n",
    "### 2.6.2网格搜索\n",
    "   + 通过修改模组若干组参数进行对比(每组参数的模型都使用交叉验证求得准确率)，比较获得最优模型\n",
    "       - 如k近邻算法中，K的参数[1,2,3,4,5],那么就分别对这个参数进行交叉验证获取\n",
    "       - 一个算法有多个参数，如a：[2,4,6,8,10],b:[10,30,40]，那么a与b内部元素分别组合得到15组参数组\n",
    "       \n",
    "   + api sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)\n",
    "       - 对估计器的指定参数值进行详尽搜索\n",
    "       - estimator 估计器对象（如k近邻中实例的knn）\n",
    "       - param_grid 估计器参数（dict）{\"n_neighbors\":[1,3,5]}\n",
    "       - cv 指定进行几折交叉验证\n",
    "       ***\n",
    "       - fit 输入训练集数据\n",
    "       - score 准确率\n",
    "       ***\n",
    "       - 结果：\n",
    "       - best_score_ 交叉验证中最好的结果\n",
    "       - best_estimator_ 最好的参数模型\n",
    "       - cv_results_ 每次交叉验证后测试集准确率结果和训练集准确率结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "a = fetch_20newsgroups(subset='all')\n",
    "#print(a.target)\n",
    "x = a.data\n",
    "y = a.target\n",
    "#分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer()\n",
    "x_train = tf.fit_transform(x_train)\n",
    "x_test = tf.transform(x_test)\n",
    "#print(tf.get_feature_names())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  7  3 ... 13 15  3]\n",
      "0.8452886247877759\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.90      0.65      0.75       228\n",
      "           comp.graphics       0.87      0.74      0.80       235\n",
      " comp.os.ms-windows.misc       0.93      0.76      0.84       265\n",
      "comp.sys.ibm.pc.hardware       0.68      0.92      0.78       236\n",
      "   comp.sys.mac.hardware       0.92      0.86      0.89       243\n",
      "          comp.windows.x       0.92      0.86      0.89       254\n",
      "            misc.forsale       0.95      0.64      0.76       259\n",
      "               rec.autos       0.86      0.92      0.89       239\n",
      "         rec.motorcycles       0.95      0.95      0.95       237\n",
      "      rec.sport.baseball       0.93      0.97      0.95       240\n",
      "        rec.sport.hockey       0.94      0.98      0.96       253\n",
      "               sci.crypt       0.82      0.97      0.89       256\n",
      "         sci.electronics       0.88      0.82      0.85       228\n",
      "                 sci.med       0.97      0.93      0.95       245\n",
      "               sci.space       0.85      0.97      0.90       231\n",
      "  soc.religion.christian       0.53      0.98      0.69       245\n",
      "      talk.politics.guns       0.76      0.96      0.85       235\n",
      "   talk.politics.mideast       0.89      0.98      0.93       233\n",
      "      talk.politics.misc       0.99      0.59      0.74       200\n",
      "      talk.religion.misc       0.93      0.17      0.29       150\n",
      "\n",
      "               micro avg       0.85      0.85      0.85      4712\n",
      "               macro avg       0.87      0.83      0.83      4712\n",
      "            weighted avg       0.87      0.85      0.84      4712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mlt = MultinomialNB(alpha = 1.0) \n",
    "mlt.fit(x_train,y_train)\n",
    "y_predict = mlt.predict(x_test)\n",
    "print(y_predict)\n",
    "print(mlt.score(x_test,y_test))\n",
    "\n",
    "# 分类模型的精确率与召回率\n",
    "from sklearn.metrics import classification_report \n",
    "cv = classification_report(y_test,y_predict,target_names=a.target_names)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的文章类别为： [ 2 17  1 ... 11  4 17]\n",
      "准确率为： 0.8448641765704584\n"
     ]
    }
   ],
   "source": [
    "def naviebayes():\n",
    "    news = fetch_20newsgroups(subset='all')\n",
    "    # 进行数据分割\n",
    "    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25)\n",
    "    # 对数据集进行特征抽取\n",
    "    tf = TfidfVectorizer()\n",
    "    # 以训练集当中的词的列表进行每篇文章重要性统计['a','b','c','d']\n",
    "    x_train = tf.fit_transform(x_train)\n",
    "    #print(tf.get_feature_names())\n",
    "    x_test  = tf.transform(x_test)\n",
    "    # 进行朴素贝叶斯算法的预测\n",
    "    mlt = MultinomialNB(alpha=1.0)\n",
    "    #print(x_train.toarray())\n",
    "    mlt.fit(x_train, y_train)\n",
    "    y_predict = mlt.predict(x_test)\n",
    "    print(\"预测的文章类别为：\", y_predict)\n",
    "    # 得出准确率\n",
    "    print(\"准确率为：\", mlt.score(x_test, y_test))\n",
    "    #print(\"每个类别的精确率和召回率：\", classification_report(y_test, y_predict, target_names=news.target_names))\n",
    "    return None\n",
    "naviebayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最合适的模型准确率 0.4829787234042553\n",
      "交叉验证最好结果： 0.47012925598991173\n",
      "合适的模型： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "每个超参数每次交叉验证的结果： {'mean_fit_time': array([0.01878405, 0.01400089, 0.02198892, 0.02918868, 0.02360063]), 'std_fit_time': array([0.00719081, 0.00725732, 0.00657305, 0.00714978, 0.00695012]), 'mean_score_time': array([0.18549166, 0.19499321, 0.34744725, 0.46653771, 0.33780079]), 'std_score_time': array([0.02300317, 0.08041587, 0.08903482, 0.0630481 , 0.08994923]), 'param_n_neighbors': masked_array(data=[1, 3, 5, 7, 10],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}, {'n_neighbors': 10}], 'split0_test_score': array([0.41400304, 0.42694064, 0.44939117, 0.44710807, 0.45471842]), 'split1_test_score': array([0.41312741, 0.44092664, 0.45096525, 0.45752896, 0.45868726]), 'split2_test_score': array([0.45154963, 0.44762652, 0.47430365, 0.47861907, 0.4688113 ]), 'split3_test_score': array([0.4475693 , 0.45118521, 0.46484532, 0.4700683 , 0.47930896]), 'split4_test_score': array([0.45929276, 0.47368421, 0.48067434, 0.48766447, 0.49095395]), 'mean_test_score': array([0.43663304, 0.44766709, 0.46374527, 0.46784363, 0.47012926]), 'std_test_score': array([0.0196326 , 0.01517976, 0.01239287, 0.0145043 , 0.01326345]), 'rank_test_score': array([5, 4, 3, 2, 1]), 'split0_train_score': array([1.        , 0.66799205, 0.61520875, 0.5861829 , 0.55675944]), 'split1_train_score': array([1.        , 0.66300257, 0.61279461, 0.583878  , 0.55189146]), 'split2_train_score': array([1.        , 0.65894072, 0.60933031, 0.58220732, 0.54808166]), 'split3_train_score': array([1.        , 0.66035886, 0.61231493, 0.58005687, 0.54907344]), 'split4_train_score': array([1.        , 0.65785881, 0.6098869 , 0.57673557, 0.54816693]), 'mean_train_score': array([1.        , 0.6616306 , 0.6119071 , 0.58181213, 0.55079459]), 'std_train_score': array([0.        , 0.00361744, 0.0021249 , 0.00323667, 0.00328663])}\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索获取k近邻算法最优K模型\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./facebook_csv/train.csv')#,nrows=2000000)\n",
    "\n",
    "data = data.query('x > 1.0 & x < 1.25 &y >2.5 & y<2.75')\n",
    "#print(data)\n",
    "timevalue = pd.to_datetime(data['time'],unit='s')\n",
    "time = pd.DatetimeIndex(timevalue)\n",
    "data['day'] = time.day\n",
    "data['hour'] = time.hour\n",
    "data['weekday'] = time.weekday\n",
    "data = data.drop(['time'],axis=1)\n",
    " \n",
    "num = data.groupby(by='place_id').count()\n",
    " \n",
    "tf = num[num.row_id>3].reset_index()#将分组依据重新划分为分组后的列元素\n",
    " \n",
    "data = data[data['place_id'].isin(tf.place_id)]#保留符合条件的地点id\n",
    " \n",
    "y = data['place_id']\n",
    "x = data.drop(['place_id'],axis=1)\n",
    "\n",
    "x = x.drop(['row_id'],axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.fit_transform(x_test)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(algorithm='auto')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param ={\"n_neighbors\":[1,3,5,7,10]}\n",
    "gs = GridSearchCV(knn,param_grid=param,cv=5)\n",
    " \n",
    "#录入数据\n",
    "gs.fit(x_train,y_train)\n",
    "#得到测试结果\n",
    "#y_predict = gc.predict(x_test)\n",
    "\n",
    "\n",
    "#准确度\n",
    "print(\"最合适的模型准确率\",gs.score(x_test,y_test))\n",
    "print(\"交叉验证最好结果：\",gs.best_score_)\n",
    "print(\"合适的模型：\",gs.best_estimator_)\n",
    "print(\"每个超参数每次交叉验证的结果：\",gs.cv_results_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 决策树、随机森林\n",
    "\n",
    "*  通过对特征进行判断进行分类 \n",
    "\n",
    "## 3.1 信息论基础   \n",
    "   * 香农 信息论创始人，信息的单位：比特\n",
    "   如共32支球队，每次猜一支球队，只知道胜或败，每次选取区间都是之前的一半，只需要5次就可以获得获胜球队\n",
    "   香农对此得到的解法为：\n",
    "   > H = -(p1㏒p1+p2㏒p2+...+p32㏒p32)  \n",
    "   H 为信息熵，单位比特 \n",
    "   pi 均为 1/32，但在一定条件下，每支球队的获胜概率不一定都一样，如巴西等国家，此时的H就会小于5\n",
    "   \n",
    "  信息熵公式为：\n",
    "  >    H(X) = ∑<sub>x∈X</sub>P(x)㏒P(x)  \n",
    " \n",
    "  信息熵越大，不确定性越大\n",
    "   ### 3.1.1 信息增益\n",
    "   * 因为决策树是使用一颗类似二叉树的样子对特征进行判断分类的，所以需要通过一些手段对特征进行权重计算，区别出现后比较的特征，这一过程需要使用信息增益来判断\n",
    "   * 特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件H(D|A)之差：\n",
    "   > g(D,A) = H(D) - H(D | A)\n",
    "   \n",
    "   信息熵：\n",
    "$$\n",
    "H(D) =- \\sum_{k=1}^{k} \\frac{|C_{k}|}{|D|}㏒\\frac{|C_{k}|}{|D|} \n",
    "$$\n",
    "\n",
    "   条件熵：\n",
    "$$  \n",
    "H(D|A) = \\sum_{i=1}^{n} \\frac{|D_{i}|}{|D|} \\sum_{k=1}^{K}\\frac{|D_{ik}|}{|D_{i}|}㏒\\frac{|D_{ik}|}{|D_{i}|} \n",
    "$$\n",
    "   - C<sub><i>k</i></sub>表示属于某类别的样本数。\n",
    "   - 真正计算时，条件熵又等于 P(F1)H(F1)+P(F2)H(F2)+P(F3)H(F3)+...\n",
    "       - 也即，计算这个特征值内每个分类的概率与信息熵，\n",
    "       - 如3.1.2内的年龄，则年龄下青年的概率为5/15，H(青年)=-(2/5 ㏒(2/5)+3/5 ㏒(3/5)),此处都是对“类别”列进行计算的信息熵，青年条件下，对应了5个类别值，2个为“是”\n",
    "   - 依次计算其他列对“类别”的信息增益\n",
    "\n",
    "   ### 3.1.2 案例演示\n",
    "   \n",
    "ID  | 年龄 |有工作|有房子|信贷情况|类别 \n",
    ":-: | :-:  | :-:  | :-:  | :-:    | :-:\n",
    "1   | 青年 | 否   |否    |一般    |否 |\n",
    "2   | 青年 | 否   |否    |好      |否 |\n",
    "3   | 青年 | 是   |否    |好      |是 |\n",
    "4   | 青年 | 是   |是    |一般    |是 |\n",
    "5   | 青年 | 否   |否    |一般    |否 |\n",
    "6   | 中年 | 否   |否    |一般    |否 |\n",
    "7   | 中年 | 否   |否    |好      |否 |\n",
    "8   | 中年 | 是   |是    |好      |是 |\n",
    "9   | 中年 | 否   |是    |非常好  |是 |\n",
    "10  | 中年 | 否   |是    |非常好  |是 |\n",
    "11  | 老年 | 否   |是    |非常好  |是 |\n",
    "12  | 老年 | 否   |是    |好      |是 |\n",
    "13  | 老年 | 是   |否    | 好     |是 |\n",
    "14  | 老年 | 是   |否    |非常好  |是 |\n",
    "15  | 老年 | 否   |否   |一般     |否 |\n",
    "\n",
    "\n",
    "***\n",
    "           \n",
    "   * 1    因为此案例是判断其他列值，然后将结果分类为“类别”列的值内，最后一列的类别就是分类结果，因此对于“类别”列的信息值为\n",
    "           H(类别) = -(9/15))㏒(9/15)+(6/15))㏒(6/15) ≈0.9\n",
    "   * 2    然后计算出ID，类别列外的其他列的信息增益值，即计算g(类别，年龄),g(类别，有工作),g(类别，有房子),g(类别，信贷情况)\n",
    "       + H(类别|年龄) = P(青年)H(青年)+P(中年)H(中年)+P(老年)H(老年)\n",
    "                   = 5/15 * (3/5㏒(3/5)+2/5㏒(2/5) )+ 5/15 * (2/5㏒(2/5)+3/5㏒(3/5) ) + 5/15 * (4/5㏒(4/5)+1/5㏒(1/5) )\n",
    "   * 3    g(类别，年龄) = H(类别) - H(类别|年龄)\n",
    "       + 依次计算出 g(类别，年龄),g(类别，有工作),g(类别，有房子),g(类别，信贷情况)\n",
    "   * 4 比较这四个信息增益，这个大小顺序即为决策树的分类依据\n",
    "***  \n",
    "### 3.2 分类依据使用算法\n",
    "- ID3 信息增益最大准则\n",
    "- C4.5 信息增益比最大准则\n",
    "- **CART**  划分更加仔细，多在sklearn中使用\n",
    "   - 回归树：平方误差最小\n",
    "   - 分类树：基尼系数最小准则，在sklearn中可以选择划分原则\n",
    "       \n",
    "### 3.3 API\n",
    "- sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)\n",
    "    - 决策树分类器\n",
    "    - criterion 默认是‘gini’系数，可以修改为信息增益熵‘entropy’\n",
    "    - max_depth 树的深度\n",
    "    - random——state 随机数种子\n",
    "    ***\n",
    "    - method：\n",
    "        - decision_path：返回决策树路径\n",
    "        \n",
    "### 3.4 案例\n",
    "- [泰坦尼克号数据](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt)\n",
    "    - 列名\"row.names\",\"pclass\",\"survived\",\"name\",\"age\",\"embarked\",\"home.dest\",\"room\",\"ticket\",\"boat\",\"sex\"\n",
    "    - 其中\"pclass\",\"survived\",\"age\",\"embarked\",\"home.dest\",\"room\",\"ticket\",\"boat\",\"sex\"是有影响的特征值，age列有缺失\n",
    "    - \n",
    "### 3.5 决策树的结构，本地保存\n",
    "- sklearn.tree.export_graphviz() 该函数能够导出DOT格式\n",
    "    - tree.export_graphviz(etimator,out_file='tree.dot',feature_names=['',''])\n",
    "    - 安装能够将dot转换为pdf，png的软件\n",
    "    \n",
    "    - 下载安装graphviz，将bin目录添加进path，命令找到导出的dot文件，然后：即可得到对应的png图片\n",
    "        - dot -Tpng tree.dot -o sample.png\n",
    "        \n",
    "### 3.6 优缺点与改进\n",
    "+ 理解简单与解释，数目可视化\n",
    "+ 数据准备少，基本不需要归一化数据\n",
    "***\n",
    "+ 决策树学习者可创建不能很好的地推广数据的过于复杂的树，--过拟合\n",
    "***\n",
    "+ 减枝cart算法（sklearn实现，与随机森林，参数调优有关）\n",
    "+ 随机森林\n",
    "\n",
    "### 3.7 集成学习方法\n",
    "+ 通过建立多个模型组合来解决单一预测问题\n",
    "+ 原理：生成多个分类器/模型，各自独立学习和做出预测，最后结合成单预测，因此优于任何一个单分类的做出预测\n",
    "\n",
    "### 3.8 随机森林\n",
    "+ 机器学习中，包含多个决策树的分类器，并输出的类别是由个别树输出的类别的众数而定\n",
    "    - 单个树建立：\n",
    "        - 随机在N个样本当中选择一个样本，重复N次（样本有可能重复，）\n",
    "        - 随机在M个特征当中选出m个特征，m（n<m）取值\n",
    "        - 此时的决策树使用的数据集基本上来说就是一个特殊的数据集，数据量为N，特征m个\n",
    "        \n",
    "    - 建立10棵决策树，样本，特征也是不一样的\n",
    "    \n",
    "      ***\n",
    "    - 随即森林是随机而有放回的抽样\n",
    "       - 随机这样每棵树的训练集都是不同的，训练出的分类结果也是不同的\n",
    "       - 如果不放回的抽样，每棵树的训练样本均不同， 且无交际，每棵树都是有偏的，片面的\n",
    "       - 随机森林的分类结果取决于多棵树（弱分类器）的投票表决\n",
    "+ api\n",
    "    - sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None)\n",
    "        - 随机森林分类器\n",
    "        - n_estimators=10，integer，optional（defalut=10）森林里的数目数量，120，200，300，500，800，1200\n",
    "        - criterion='gini' string 可选，默认gini，分割特征的测量方法\n",
    "        - max_depth=None integer或None，可选默认为无，树的最大深度\n",
    "        - bootstrap=True boolean,可选默认为True，是否在构建树时使用放回抽样 \n",
    "        - random_state=None\n",
    "        - max_features = 'auto' 每个决策树的最大特征数量\n",
    "            - auto 为n_features的开方\n",
    "            - sqrt 与auto一致\n",
    "            - log2 log2（n_features）\n",
    "            - None 为n_features\n",
    "+ 优缺点\n",
    "    - 当前算法中，具有极好准确率\n",
    "    - 能有效运行在大数据集上\n",
    "    - 处理具有高维特征的输入样本，而且不需要降维\n",
    "    - 能够评估各个特征在分类问题上的重要性\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titan = pd.read_csv(\"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt\")\n",
    "print(titan.head(5))\n",
    "print(titan.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']\n",
      "[[31.19418104  0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  0.          1.          0.          1.          0.        ]\n",
      " ...\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  0.          0.          1.          1.          0.        ]\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]]\n",
      "[[32.          0.          1.          0.          0.          1.        ]\n",
      " [64.          1.          0.          0.          0.          1.        ]\n",
      " [56.          1.          0.          0.          1.          0.        ]\n",
      " ...\n",
      " [26.          0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  1.          0.          0.          0.          1.        ]\n",
      " [18.          0.          0.          1.          0.          1.        ]]\n",
      "0.7872340425531915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\pandas\\core\\generic.py:6130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "x = titan[['pclass','age','sex']]\n",
    "y = titan[['survived']]\n",
    "x['age'].fillna(x['age'].mean(),inplace=True)\n",
    "#print(x)\n",
    "#分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "# one hot 处理\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dict = DictVectorizer(sparse=False)\n",
    "x_train = dict.fit_transform(x_train.to_dict(orient='records'))# to_dict将数组转换为字典\n",
    "print(dict.get_feature_names())\n",
    "print(x_train)\n",
    "x_test = dict.transform(x_test.to_dict(orient='records'))\n",
    "print(x_test)\n",
    "\n",
    "from  sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "dec = DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)\n",
    "dec.fit(x_train,y_train)\n",
    "print(dec.score(x_test,y_test))\n",
    "\n",
    "export_graphviz(dec,out_file='tree.dot',feature_names=dict.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林\n",
    "import time\n",
    "a = time.now()\n",
    "print(a)\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "x = titan[['pclass','age','sex']]\n",
    "y = titan[['survived']]\n",
    "x['age'].fillna(x['age'].mean(),inplace=True)\n",
    "#print(x)\n",
    "#分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "# one hot 处理\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dict = DictVectorizer(sparse=False)\n",
    "x_train = dict.fit_transform(x_train.to_dict(orient='records'))# to_dict将数组转换为字典\n",
    "print(dict.get_feature_names())\n",
    "\n",
    "\n",
    "x_test = dict.transform(x_test.to_dict(orient='records'))\n",
    "print(x_test)\n",
    "\n",
    "rf = RandomForestClassifier( )\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param ={n_estimators:[100,120,300],max_depth:[5,8,15,25,30] }\n",
    "gs = GridSearchCV(rf,param_grid=param,cv=2)\n",
    "gs.fit(x_train,y_train)\n",
    "print(\"准确率：\",gc.score(x_test,y_test)) \n",
    "print(\"查看选择的参数模型：\",gc.best_params) \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# export_graphviz(dec,out_file='tree.dot',feature_names=dict.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "a = time.time()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
