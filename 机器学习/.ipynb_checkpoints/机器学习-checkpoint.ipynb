{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一部分\n",
    "***\n",
    "\n",
    "# 机器学习\n",
    "数据文件csv\n",
    "+ 数据大，使用mysql存储有性能瓶颈，读取速度\n",
    "\n",
    "+ pandas：读取工具，\n",
    "+ numpy：释放python的GIL\n",
    "\n",
    "+ 数据集结构：特征值+目标值（有些时候可以没有）；DataFrame格式\n",
    "\n",
    "+ 特征工程：将原始数据转换为更好的代表预测模型的潜在问题的特征的过程，从而提高对位置数据的预测准确性\n",
    "\n",
    "+ 使用Scikit-learn 库进行数据处理;文档完善\n",
    "\n",
    "  `` pip install sklearn`` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据的特征化\n",
    "### 1.1 字典，数组特征值化;\n",
    "sklearn.feature_extraction.DictVectorizer\n",
    "+ DictVectorizer.fit_transform(x)，传入字典或包含字典的迭代器返回sparse矩阵，\n",
    "+ .inverse_transform(x)，传入array数组或sparse矩阵，返回转换之前的数据格式\n",
    "+ .get_feature_names()返回类别名称，即如果是字典时，是字典的key，或是数组的列名\n",
    "+ .transform(x)按原本标准转换\n",
    "\n",
    "### 1.2  对文本数据进行特征值化\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "+ CountVectorizer()返回词频矩阵\n",
    "+ .fit_transform(x)，传入文本或包含文本字符串的迭代器返回sparse矩阵，\n",
    "+ .inverse_transform(x)，传入array数组或sparse矩阵，返回转换之前的数据格式\n",
    "+ .get_feature_names()返回类别名称， "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city=上海', 'city=北京', 'city=深圳', 'temperature']\n",
      "[{'city=北京': 1.0, 'temperature': 100.0}, {'city=上海': 1.0, 'temperature': 80.0}, {'city=深圳': 1.0, 'temperature': 99.0}]\n",
      "[[  0.   1.   0. 100.]\n",
      " [  1.   0.   0.  80.]\n",
      " [  0.   0.   1.  99.]]\n"
     ]
    }
   ],
   "source": [
    "#字典，数组特征值化;\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def dictvec(dic):\n",
    "    dict = DictVectorizer(sparse=False)#参数sparse默认是true，设置为false则返回一个矩阵\n",
    "    data = dict.fit_transform(dic)\n",
    "    print(dict.get_feature_names())\n",
    "    print(dict.inverse_transform(data))\n",
    "    return data\n",
    "\n",
    "dic = [{\"city\":'北京',\"temperature\":100},{\"city\":'上海',\"temperature\":80},{\"city\":'深圳',\"temperature\":99}]\n",
    "da = dictvec(dic)\n",
    "print(da)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 输出矩阵的列信息：['city=上海', 'city=北京', 'city=深圳', 'temperature']\n",
    "data的内容：打印矩阵，需要设置dict = DictVectorizer(sparse=False)#参数sparse默认是true，设置为false则返回一个矩阵\n",
    "[[  0.   1.   0. 100.]\n",
    " [  1.   0.   0.  80.]\n",
    " [  0.   0.   1.  99.]] one-hot编码：将有类别的（字符串形式出现的特征值）特征值名进行编码，也就是针对原字典的key，建立bool列，此处对city建立三个列，分别为上海，北京，深圳，然后dic字典内第一个为北京，因此这个（0，1）位置为1，第二个位上海，因此（1，0）为1，对于temperature特征则直接赋值\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'like', 'love', 'many', 'people', 'the', 'world']\n",
      "=================\n",
      "[array(['people', 'many', 'love'], dtype='<U6'), array(['world', 'the', 'like', 'just', 'love'], dtype='<U6')]\n",
      "[[0 0 1 2 1 0 0]\n",
      " [1 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#特征文本特征抽取\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dictvec(st):\n",
    "    cv = CountVectorizer()\n",
    "    data = cv.fit_transform(st)\n",
    "    print(cv.get_feature_names())\n",
    "    print('=================')\n",
    "    print(cv.inverse_transform(data))\n",
    "    return data\n",
    "\n",
    "st =[\"i love many many people\",\"just like i love the world\"]\n",
    "da = dictvec(st)\n",
    "print(da.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['just', 'like', 'love', 'many', 'people', 'the', 'world'],矩阵列名，统计所有文章中所有的词，重复的只看做一次,单个英文字母不处理统计\n",
    "输出的特征化矩阵：输出矩阵形式需要：使用toarray()方法\n",
    "[[0 0 1 2 1 0 0] \n",
    " [1 1 1 0 0 1 1]]  one-hot,建立，按文章的顺序，来统计文章中每一个词汇在对应列下出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['不予', '人生', '岁月', '清风', '漫漫', '长进']\n",
      "=================\n",
      "[array(['长进', '漫漫', '人生'], dtype='<U2'), array(['不予', '岁月', '清风'], dtype='<U2')]\n",
      "[[0 1 0 0 1 1]\n",
      " [1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#中文处理时，默认不支持，只会根据符号（表单符号，空格）来划分，对单个汉字同样不处理\n",
    "#可以使用jieba分词来操作，将文章分词后使用一个空格连接成一个str\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dictvec(st):\n",
    "    cv = CountVectorizer()\n",
    "    data = cv.fit_transform(st)\n",
    "    print(cv.get_feature_names())\n",
    "    print('=================')\n",
    "    print(cv.inverse_transform(data))\n",
    "    return data\n",
    "import jieba\n",
    "st =[\"人生漫漫，好不。长进\",\"清风岁月不予心\"]\n",
    "st = [' '.join(list(jieba.cut(i))) for i in st]\n",
    "da = dictvec(st)\n",
    "print(da.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 中性词的处理：\n",
    "因为不同文本里的中性词，即如代词，介词，连词一类对文章本身不具有特别的情感色彩的界定，但这些词又占有一定比例（不同文本内这些词比例一样时，不能说两篇文本是相似的，因此需要其他解决方法）\n",
    "文本抽取方式：\n",
    "1. tf-idf\n",
    "  tf term frequency 词频   \n",
    "  idf inverse document frequency 逆文档频率  ㏒(总文档数量/该词出现的文档数量)，即一个词在多个文档中出现过；  \n",
    "  重要性 tf*idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['不予', '人生', '因此', '岁月', '清风', '漫漫', '长进']\n",
      "=================\n",
      "[array(['人生', '漫漫', '长进'], dtype='<U2'), array(['清风', '岁月', '不予', '因此'], dtype='<U2')]\n",
      "[[0.         0.57735027 0.         0.         0.         0.57735027\n",
      "  0.57735027]\n",
      " [0.5        0.         0.5        0.5        0.5        0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#中性词的处理\n",
    "#sklearn.feature_extraction.text.TfidVectorizer\n",
    " \n",
    "#TfidVectorizer()\n",
    "#     返回词的权重矩阵\n",
    "#fit_transform(x)，传入文本或包含文本字符串的迭代器返回sparse矩阵，\n",
    "#.inverse_transform(x)，传入array数组或sparse矩阵，返回转换之前的数据格式\n",
    "#.get_feature_names()返回单词列表，\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dictvec(st):\n",
    "    cv = TfidfVectorizer()\n",
    "    data = cv.fit_transform(st)\n",
    "    print(cv.get_feature_names())\n",
    "    print('=================')\n",
    "    print(cv.inverse_transform(data))\n",
    "   \n",
    "    return data\n",
    "import jieba\n",
    "st =[\"人生漫漫，好不。长进\",\"清风岁月不予心的我，因此\"]\n",
    "st = [' '.join(list(jieba.cut(i))) for i in st]\n",
    "da = dictvec(st)\n",
    "print(da.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征预处理：\n",
    "### 2.1 数值类型的数据：\n",
    "+ 归一化\n",
    "    - 通过对原始数据进行变换把数据映射到（默认为【0，1】）之间\n",
    "    - 公式：\n",
    "      ``X` = （x-min） /(max-min)   X`` = X`*(mx-mi)+mi``  \n",
    "    \n",
    "      作用与每一列，max为一列最大值，min为列最小值，X``为最终结果（按列对应位置赋值），mx，mi为指定区间值，默认mx=1，mi=0\n",
    "\n",
    "    - 作用：数据集里特征有多个，且针对个别列值特别大，如A列均在5000左右，B列在10以内，则数据相关不一致，经过归一化后，均落点在（0，1）内方便计算，使得某一个特征对最终结果不会造成更大影响\n",
    "    - 异常点：对最大值最小值影响太大，在公式中对结果影响过大（新增某一个点）\n",
    "      - 只适合传统精确小数据场景\n",
    "\n",
    "+ 标准化：\n",
    "    - sklearn.preprocessing.StandardScaler\n",
    "    - 通过对原始数据进行变换到均值在0，方差为1的范围内\n",
    "    - 公式  \n",
    "        X` = (x-mean)/σ\n",
    "        - 作用与每一列，\n",
    "        - mean为平均值，σ为标准差，var为方差\n",
    "        - var = (x1=mean)²+(x2+mean)²+...  /n\n",
    "        - σ = √var    \n",
    "        \n",
    "    - 异常点的影响因为有数据量的存在，对平均值影响较小，方差改变也较小\n",
    "    - 适合样本多的情况下\n",
    "\n",
    "+ 缺失值\n",
    "    - 删除：每列或行数据缺失达到一定比例，放弃该列/行\n",
    "    - 填补：根据行，列，均值，中位数填补\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.        ]\n",
      " [0.         1.         1.         0.83333333]\n",
      " [0.5        0.5        0.6        1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "k = mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,13,46]])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22474487 -1.22474487 -1.29777137 -1.3970014 ]\n",
      " [-1.22474487  1.22474487  1.13554995  0.50800051]\n",
      " [ 0.          0.          0.16222142  0.88900089]]\n",
      "[75.          3.         12.66666667 43.66666667]\n",
      "[150.           0.66666667   4.22222222   6.88888889]\n"
     ]
    }
   ],
   "source": [
    "#标准化\n",
    "#StandardScaler()\n",
    "#     \n",
    "#fit_transform(x)，传入numpy array格式数据返回sparse矩阵，\n",
    "#.inverse_transform(x)，传入array数组或sparse矩阵，返回转换之前的数据格式\n",
    "#.get_feature_names()返回单词列表，\n",
    "#.var_原始数据每列特征的方差\n",
    "#.mean_每列特征均值\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mm = StandardScaler()\n",
    "k = mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,13,46]])\n",
    "print(k)\n",
    "print(mm.mean_)\n",
    "print(mm.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90.   3.5 10.  40. ]\n",
      " [60.   4.  11.5 45. ]\n",
      " [75.   3.  13.  46. ]]\n",
      "[[90, nan, 10, 40], [60, 4, nan, 45], [75, 3, 13, 46]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#缺失值\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "import numpy as np\n",
    "mm = Imputer(missing_values=\"NaN\",strategy='mean',axis=0)\n",
    "li = [[90,np.nan,10,40],[60,4,np.nan,45],[75,3,13,46]]\n",
    "k = mm.fit_transform(li)\n",
    "print(k)\n",
    "print(li )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据降维\n",
    "### 3.1 特征数量减少  \n",
    "方式一，特征选择：sklearn.feature_selection\n",
    "  >  从所有特征中选择部分特征作为训练集特征，对于特征值在选择前后可以改变   \n",
    "    冗余：个别特征是不需要的，浪费计算性能   \n",
    "    噪声：部分特征对预测结果有影响   \n",
    "    主要方法：  \n",
    "    Filter 过滤 针对方差（低）的过滤  \n",
    "    Embedded 嵌入 ，正则化，决策树，神经网络  \n",
    "    Wrapper包裹  \n",
    "    其他方法：\n",
    "    神经网络  \n",
    "    \n",
    "方式二，主成分分析：sklearn.decompostion\n",
    "   > PCA:分析简化数据集技术  \n",
    "    数据维数压缩。尽可能降低原数据维数，损失少量信息  \n",
    "    可以削减回归分析或具类分析中特征数量  \n",
    "    高纬度数据容易出现的问题：  \n",
    "        特征间可能存在相关性      \n",
    "    如一个二维的数据，表现上为一个二维数轴上的点，降维则可以将所有点投影到数轴的y=x线上，这样点的个数不变，但是维度降低了。\n",
    "    公式：Y = PX \n",
    "    矩阵运算得到:  \n",
    "    ``P= （1/√2  1/√2\n",
    "        -1/√2 1/√2）\n",
    "    Y = (1/√2  1/√2)    (-1 -1 0 2 0 \n",
    "                           -2 0  0 1 0)   =(-3/√2  -1/√2 0 3/√2  -1/√2)``\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90.  10.  40. ]\n",
      " [60.  11.5 45. ]\n",
      " [75.  13.  46. ]]\n"
     ]
    }
   ],
   "source": [
    "#Filter\n",
    "\n",
    "#VarianceThreshold(threshold=0.0)\n",
    "#.fit_transform(X)，传入numpy。array格式数据，返回训练集差异低于threshold的特征将被删除，默认保留所有非零方差特征，删除所有样本中具有相同值的特征\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold(threshold=1.0)\n",
    "v = vt.fit_transform([[90,4,10,40],\n",
    "                     [60,4,11.5,45],\n",
    "                     [75,4,13,46]])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 主成分分析PCA\n",
    "* PCA(n_components=None)数据分解到较低维数空间 n_components: 小数 0-1 之间，保留的特征值比例，整数：减少到的特征值数量\n",
    "* PCA.fit_transform(X)  传入numpy.array个数数据，返回指定维度的array  \n",
    "\n",
    "`` from sklearn.decomposition import PCA\n",
    "    p = PCA(n_components=0.9)\n",
    "data = p.fit_transform([[90,4,10,40],\n",
    "                     [60,4,11.5,45],\n",
    "                     [75,4,13,46]])\n",
    "print(data)  ``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = pd.read_csv('./market_csv/order_products__prior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('./market_csv/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv('./market_csv/orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisles = pd.read_csv('./market_csv/aisles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mg = pd.merge(prior,products,on=['product_id','product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mg = pd.merge(_mg,orders,on=['order_id','order_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = pd.merge(_mg,aisles,on=['aisle_id','aisle_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  product_id  add_to_cart_order  reordered        product_name  \\\n",
      "0         2       33120                  1          1  Organic Egg Whites   \n",
      "1        26       33120                  5          0  Organic Egg Whites   \n",
      "2       120       33120                 13          0  Organic Egg Whites   \n",
      "3       327       33120                  5          1  Organic Egg Whites   \n",
      "4       390       33120                 28          1  Organic Egg Whites   \n",
      "5       537       33120                  2          1  Organic Egg Whites   \n",
      "6       582       33120                  7          1  Organic Egg Whites   \n",
      "7       608       33120                  5          1  Organic Egg Whites   \n",
      "8       623       33120                  1          1  Organic Egg Whites   \n",
      "9       689       33120                  4          1  Organic Egg Whites   \n",
      "\n",
      "   aisle_id  department_id  user_id eval_set  order_number  order_dow  \\\n",
      "0        86             16   202279    prior             3          5   \n",
      "1        86             16   153404    prior             2          0   \n",
      "2        86             16    23750    prior            11          6   \n",
      "3        86             16    58707    prior            21          6   \n",
      "4        86             16   166654    prior            48          0   \n",
      "5        86             16   180135    prior            15          2   \n",
      "6        86             16   193223    prior             6          2   \n",
      "7        86             16    91030    prior            11          3   \n",
      "8        86             16    37804    prior            63          3   \n",
      "9        86             16   108932    prior            16          1   \n",
      "\n",
      "   order_hour_of_day  days_since_prior_order aisle  \n",
      "0                  9                     8.0  eggs  \n",
      "1                 16                     7.0  eggs  \n",
      "2                  8                    10.0  eggs  \n",
      "3                  9                     8.0  eggs  \n",
      "4                 12                     9.0  eggs  \n",
      "5                  8                     3.0  eggs  \n",
      "6                 19                    10.0  eggs  \n",
      "7                 21                    12.0  eggs  \n",
      "8                 12                     3.0  eggs  \n",
      "9                 13                     3.0  eggs  \n"
     ]
    }
   ],
   "source": [
    "print(me.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.crosstab(me['user_id'],me['aisle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cross.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.9)#保留百分之九十特征\n",
    "data = pca.fit_transform(cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.42156587e+01  2.42942720e+00 -2.46636975e+00 ...  6.86800336e-01\n",
      "   1.69439402e+00 -2.34323022e+00]\n",
      " [ 6.46320806e+00  3.67511165e+01  8.38255336e+00 ...  4.12121252e+00\n",
      "   2.44689740e+00 -4.28348478e+00]\n",
      " [-7.99030162e+00  2.40438257e+00 -1.10300641e+01 ...  1.77534453e+00\n",
      "  -4.44194030e-01  7.86665571e-01]\n",
      " ...\n",
      " [ 8.61143331e+00  7.70129866e+00  7.95240226e+00 ... -2.74252456e+00\n",
      "   1.07112531e+00 -6.31925661e-02]\n",
      " [ 8.40862199e+01  2.04187340e+01  8.05410372e+00 ...  7.27554259e-01\n",
      "   3.51339470e+00 -1.79079914e+01]\n",
      " [-1.39534562e+01  6.64621821e+00 -5.23030367e+00 ...  8.25329076e-01\n",
      "   1.38230701e+00 -2.41942061e+00]]\n",
      "(206209, 27)\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    i   j   k   l   m\n",
      "a   1   2   3   4   5\n",
      "b   6   7   8   9  10\n",
      "c  11  12  13  14  15\n",
      "d  16  17  18  19  20\n",
      "e  21  22  23  24  25\n",
      "    i   j     k   l   m\n",
      "a   1   2  jiao   4   5\n",
      "b   1   7  jiao   9  10\n",
      "c  11  12    ll  14  15\n",
      "d  16  17    18  19  20\n",
      "e  21  22  jiao  24  25\n",
      "=======\n",
      "a     1\n",
      "b     1\n",
      "c    11\n",
      "d    16\n",
      "e    21\n",
      "Name: i, dtype: int32\n",
      "=======\n",
      "k   18  jiao  ll\n",
      "i               \n",
      "1    0     2   0\n",
      "11   0     0   1\n",
      "16   1     0   0\n",
      "21   0     1   0\n",
      "=========\n",
      "i\n",
      "1     0\n",
      "11    0\n",
      "16    1\n",
      "21    0\n",
      "Name: 18, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#crosstab，交叉分组，相当于这样一个流程，从一个列表中抽取两列，A列的元素取set（）当作index,B列的元素取set()为colums，\n",
    "    #设置行，列索引时，过程如此:\n",
    "    #    设置一个全0数组，行列索引分别为A，B，然后回原数组按行为一组取A，B的值，即为新数组的一个行列的loc取值元组，如果该位置为0，则赋值1，否则累加即可。\n",
    "    #    即新数组保存了原AB两列同行间的关系，以及每一类的相同行（类别）的个数\n",
    "a = pd.DataFrame(np.arange(1,26).reshape(5,5),index=list('abcde'),columns=list('ijklm'))\n",
    "print(a)\n",
    "\n",
    "a.loc['a','k']='jiao'\n",
    "a.loc['b','k']='jiao'\n",
    "a.loc['e','k']='jiao'\n",
    "a.loc['c','k']='ll'\n",
    "a.loc['b','i']=1\n",
    "print(a)\n",
    "print('=======')\n",
    "print(a['i'])\n",
    "print('=======')\n",
    "b = pd.crosstab(a['i'],a['k'])\n",
    "print(b)\n",
    "print('=========')\n",
    "print(b[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 第二部分\n",
    "***\n",
    "# 机器学习基础\n",
    "   * 核心：算法，\n",
    "   * 基础：数据和计算\n",
    "\n",
    "## 1.1 数据类型：\n",
    "  + 离散型数据：计数数据，全都是整数，不能再被细分，也无法进一步提高精确度\n",
    "  + 连续性数据：变量可以在一定范围内取任一数，通常是非整数的，长度，时间，质量值，可被细分\n",
    "  \n",
    "## 1.2 机器学习算法分类\n",
    "  * 监督学习（预测）:特征值+目标值   \n",
    "      * 分类（离散型数据） k-近邻算法，贝叶斯分类，决策树与随机森林，逻辑回归，神经网络   \n",
    "      * 回归（连续型数据） 线性回归，岭回归   \n",
    "      * 标注 隐马尔科夫模型   \n",
    "  * 无监督学习：特征值  \n",
    "      * 聚类 k-means  \n",
    "      \n",
    "## 1.3 业务流程\n",
    "   * 建立模型：根据数据类型划分应用种类\n",
    "       >  \n",
    "          原属数据明确问题内容\n",
    "          数据基本处理：pd处理数据（缺省值，合并表）\n",
    "          特征工程（特征进行处理）\n",
    "          合适的算法预测\n",
    "          模型评估\n",
    "\n",
    "## 1.4 数据集划分\n",
    "   + 训练数据\n",
    "   + 测试数据，评估模型是否有效\n",
    "   \n",
    "## 1.5 加载数据集\n",
    " sklearn.datasets\n",
    "+ .load_*()#小规模数据集，数据包含在datasets里\n",
    "+ .fetch_*(data_home=None)#获取大规模数据集，需要网络下载，需要传入数据集下载的目录，默认为 ~/scikit_learn_data/\n",
    "返回的数据类型都是datasets.base.Bunch(字典格式),一般包含的参数值有\n",
    "   + data:特征数据数组，是[n_samples*n_features]的二位numpy.ndarray数组\n",
    "   + target：标签数组，是n_samples的一维numpy ndarray数组\n",
    "   + DESCR：数据描述\n",
    "   + feature_names：特征名，新闻数据，手写数字、回归数据集没有\n",
    "   + target_names：标签名  \n",
    "   \n",
    "  ### 1.5.1 用于分类的大数据集\n",
    "   + sklearn.datasets.fetch_20newsgroups(data_home=None,subset='train')\n",
    "        - subset:'train','all','test'可选，选择加载（没有则自动下载）的数据集为：‘训练’，‘全部’，‘测试’\n",
    "   + datasets.clear_data_home(data_home=None)\n",
    "        清除目录下的数据\n",
    "        \n",
    "  ### 1.5.2 sklearn回归数据集\n",
    "   + sklearn.datasets.load_boston()加载返回波士顿房价数据集\n",
    "   + sklearn.datasets.load_diabetes() 加载返回糖尿病数据集\n",
    "   \n",
    "## 1.6 分割数据集\n",
    "   sklearn.model_selection.train_test_split(*arrays,**options)  \n",
    "   配置参数：  \n",
    "   \n",
    "   + x:数据集特征值 \n",
    "   + y:数据集标签值\n",
    "   + test_size 测试集大小，一般为float，0-1之间\n",
    "   + random_state 随机数种子，同一个种子的采样结果一样\n",
    "   + return 训练集特征值，测试集特征值，训练标签，测试标签（默认随机取）\n",
    "    \n",
    "## 1.7转换器\n",
    "   fit_transform()：具有输入数据与转换功能\n",
    "   +     fit():只输入数据\n",
    "   +    transform()：进行数据转换\n",
    "   + transform()转换的值，只根据最近的fit()传入值来执行的\n",
    "   \n",
    "## 1.8估计器\n",
    "实现算法的API\n",
    " ### 1.8.1 分类估计器\n",
    "   + sklearn.neighbors k近邻\n",
    "   + sklearn.naive_bayes 贝叶斯\n",
    "   + sklearn.liner_model.LogisticRegression 逻辑回归\n",
    "   + sklearn.tree 决策树与随机森林  \n",
    "   \n",
    "### 1.8.2 回归估计器\n",
    "   + sklearn.liner_model.LinearRegression 线性回归\n",
    "   + sklearn.liner_model.Ridge 岭回归\n",
    "    \n",
    "### 1.8.3 估计器工作流程： \n",
    " >   ↓输入训练集  \n",
    "        fit(x_train,y_train)  \n",
    "        y_predict = predict(x_test)  \n",
    "        score(x_test,y_test) 评估结果  \n",
    "    ↑输入测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "li = load_iris()\n",
    "print(li.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "a = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#print(a.data)\n",
    "#print(a.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "bo = load_boston()\n",
    "#print(bo.target)\n",
    "#print(bo.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.4 3.9 1.7 0.4]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.6 2.5 3.9 1.1]]\n"
     ]
    }
   ],
   "source": [
    "#分割数据集\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "li = load_iris()\n",
    "data = train_test_split(li.data,li.target,test_size=0.25)\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 第三部分 针对离散数据\n",
    "***\n",
    "# 1. K-近邻算法\n",
    "+ 通过距离公式求得距离最近的特征样本\n",
    "    - 所谓K意思为，先根据一个有特征值和目标值的数据集，计算这些数据的距离与分类的关系；然后使用测试数据时，测试集内的一条数据的特征值计算距离后，取K个最相近的测试集数据，然后找出这K个测试集数据中大多数属于的类别，这个类别就是这条测试数据归属的类别。\n",
    "+ api: sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "\t- n_neighbors:int，可选，默认为5，k_neighbors查询默认使用的邻居数\n",
    "\t- algorithm：{auto,ball_tree,kd_tree,brute}计算最近邻算法\n",
    "\t- ball_tree 使用BallTree\n",
    "    - kd_tree 使用BDTree\n",
    "    - auto 尝试根据传递的fit方法的值来决定最适合的算法\n",
    "    \n",
    "+ 两个样本的距离算法：\n",
    "    欧式距离 如  \n",
    "     > `` a(a1,a2,a3),b(b1,b2,b3)   \n",
    "     distance = √((a1-b1)²+(a2-b2)²+(a3-b3)²)``  \n",
    "    \n",
    "+ 数据需要做标准化\n",
    "+ 使用：   \n",
    "\t> knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')  \n",
    "    knn.fit(x_train,y_train) 录入训练集的特征值和标签  \n",
    "    y_predict = knn.predict(x_test) 对测试集的特征集进行计算，返回预测的标签值  \n",
    "    score(x_test,y_test) 评估结果 输入测试特征值与实际的标签值，根据上一步得到的预测值进行对比得到精确度    \n",
    "    \n",
    "+ 关于k值 \n",
    "    - K值设置小时，异常点影响大 \n",
    "    - 设置大时，受k值数量（类别）波动\n",
    "+ 性能   \n",
    "    - 简单，便于理解，易于实现，无需估计参数，无需训练\n",
    "+ 懒惰算法，对测试样本分类时计算量大，内存开销大\n",
    "    - 必须指定k，k选择不当分类精度不能保证使用场景\n",
    "    \n",
    "+ 案例：\n",
    "  - 对[facebook客户位置坐标推荐住店](https://www.kaggle.com/c/facebook-v-predicting-check-ins/data)  \n",
    "  - 数据集内容：\n",
    "      - row_id: 入住编号（事件基本无关）id of the check-in event\n",
    "      - x y: 用户的坐标coordinates\n",
    "      - accuracy: 坐标精确度，location accuracy \n",
    "      - time: 时间戳timestamp\n",
    "      - place_id:目标值， id of the business, this is the target you are predicting\n",
    "      \n",
    "  - 流程：\n",
    "      - 导入数据\n",
    "          - pandas导入数据集\n",
    "      - 数据处理\n",
    "          - pandas.query\n",
    "          - pandas.to_datetime\n",
    "          - pandas.DatetimeIndex\n",
    "          - 分割 \n",
    "              - from sklearn.model_selection import train_test_split\n",
    "              - x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "      - K近邻\n",
    "          - knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "          - knn.fit(x_train,y_train)#录入数据 \n",
    "          - y_predict = knn.predict(x_test)#得到测试结果\n",
    "          - knn.score(x_test,y_test)准确度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./facebook_csv/train.csv')#,nrows=2000000)\n",
    "\n",
    "#print(data.head(10))\n",
    "#print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "600      600  1.2214  2.7023        17  6683426742    1    18        3\n",
      "957      957  1.1832  2.6891        58  6683426742   10     2        5\n",
      "4345    4345  1.1935  2.6550        11  6889790653    5    15        0\n",
      "4735    4735  1.1452  2.6074        49  6822359752    6    23        1\n",
      "5580    5580  1.0089  2.7287        19  1527921905    9    11        4\n",
      "6090    6090  1.1140  2.6262        11  4000153867    2    16        4\n",
      "6234    6234  1.1449  2.5003        34  3741484405    4    15        6\n",
      "6350    6350  1.0844  2.7436        65  5963693798    1    10        3\n",
      "7468    7468  1.0058  2.5096        66  9076695703    9    15        4\n",
      "8478    8478  1.2015  2.5187        72  3992589015    8    23        3\n"
     ]
    }
   ],
   "source": [
    "# 查找坐标范围\n",
    "data = data.query('x > 1.0 & x < 1.25 &y >2.5 & y<2.75')\n",
    "#print(data)\n",
    "timevalue = pd.to_datetime(data['time'],unit='s')\n",
    "time = pd.DatetimeIndex(timevalue)#将时间序列转换为字典\n",
    "data['day'] = time.day\n",
    "data['hour'] = time.hour\n",
    "data['weekday'] = time.weekday\n",
    "data = data.drop(['time'],axis=1)\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08274231678486997\n"
     ]
    }
   ],
   "source": [
    "num = data.groupby(by='place_id').count()\n",
    "#print(num)\n",
    "#保留三次以上的点\n",
    "tf = num[num.row_id>3].reset_index()#将分组依据重新划分为分组后的列元素\n",
    "#print(tf)\n",
    "data = data[data['place_id'].isin(tf.place_id)]#保留符合条件的地点id\n",
    "#print(data)\n",
    "y = data['place_id']\n",
    "x = data.drop(['place_id'],axis=1)\n",
    "#分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "\n",
    "#录入数据\n",
    "knn.fit(x_train,y_train)\n",
    "#得到测试结果\n",
    "y_predict = knn.predict(x_test)\n",
    "#print(y_predict)\n",
    "\n",
    "#准确度\n",
    "print(knn.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48652482269503544\n"
     ]
    }
   ],
   "source": [
    "# k近邻数据标准化\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./facebook_csv/train.csv')#,nrows=2000000)\n",
    "\n",
    "data = data.query('x > 1.0 & x < 1.25 &y >2.5 & y<2.75')\n",
    "#print(data)\n",
    "timevalue = pd.to_datetime(data['time'],unit='s')\n",
    "time = pd.DatetimeIndex(timevalue)\n",
    "data['day'] = time.day\n",
    "data['hour'] = time.hour\n",
    "data['weekday'] = time.weekday\n",
    "data = data.drop(['time'],axis=1)\n",
    " \n",
    "num = data.groupby(by='place_id').count()\n",
    " \n",
    "tf = num[num.row_id>3].reset_index()#将分组依据重新划分为分组后的列元素\n",
    " \n",
    "data = data[data['place_id'].isin(tf.place_id)]#保留符合条件的地点id\n",
    " \n",
    "y = data['place_id']\n",
    "x = data.drop(['place_id'],axis=1)\n",
    "\n",
    "x = x.drop(['row_id'],axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.fit_transform(x_test)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "\n",
    "#录入数据\n",
    "knn.fit(x_train,y_train)\n",
    "#得到测试结果\n",
    "y_predict = knn.predict(x_test)\n",
    "\n",
    "\n",
    "#准确度\n",
    "print(knn.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# 鸢尾花数据集k近邻预测\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "li = load_iris()\n",
    "print(len(li.target))\n",
    "print(len(li.data))\n",
    "x = li.data\n",
    "y = li.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "\n",
    "#录入数据\n",
    "knn.fit(x_train,y_train)\n",
    "#得到测试结果\n",
    "y_predict = knn.predict(x_test)\n",
    "#print(y_predict)\n",
    "\n",
    "#准确度\n",
    "print(knn.score(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. 朴素贝叶斯算法\n",
    "   + 特征独立  \n",
    "    \n",
    "## 2.1 概率基础\n",
    "   + 条件概率\n",
    "       - 事件A在另一事件B已发生条件下发生的概率(A1,A2相互独立)\n",
    "       > P(A|B)    \n",
    "       P(A1，A2|B)=P(A1|B)P(A2|B)\n",
    "         \n",
    "   + 联合概率\n",
    "       - 包含多个条件，且所有条件同时成立的概率\n",
    "       > P(A,B) = P(A)P(B)\n",
    "       \n",
    "## 2.2 朴素贝叶斯-贝叶斯公式\n",
    "   + 文档有多个词（有一定情感的），\n",
    "   >P(C|W) = P(W|C)P(C) / P(W)\n",
    "       - W为给定文档的特征值（频数统计，预测文档提供，）\n",
    "       - C为文档类别  \n",
    "       \n",
    "   >公式可以变换为：P(C|F1,F2,F3,F4...) = P(F1,F2,F3,F4...|C)P(C) / P(F1,F2,F3,F4...)\n",
    "       - C 可以为不同类别\n",
    "       \n",
    "       - 实际计算为：P(C|F1,F2,F3,F4...) = P(F1,F2,F3,F4...|C)P(C)\n",
    "                           =P(F1|C)P(F2|C)P(F3|C)P(F4|C)...P(C)\n",
    "           - P(F1|C) = 这个词在C类别下出现的次数 / 这个词在C类别下所有的特征词的总个数（非类别，而是各特征值对应出现的总次数）\n",
    "              - 这个词在C类别下出现的次数 ，如C为科技类别，那么训练集内，划分到科技类别的文章中这个词共出现的次数\n",
    "              - 分母为，科技类别下，所有的特征词出现的次数的总和，\n",
    "           - P(C) = 训练集中C类别下文章篇数 / 训练集总文章篇数\n",
    "           \n",
    "       - 有些时候计算结果可能是0.，则解决方法：拉普拉斯平滑系数\n",
    "       > P(F1|C) = (Ni+α) / (N+ αm)\n",
    "         - α为指定的系数，一般为1，m为训练文档中统计出的特征词个数（也就是特征词有几个类别，不是这些特征词出现的频数）\n",
    "         - Ni 为这个词在C类别下出现的次数\n",
    "         - N 科技类别下，所有的特征词出现的次数的总和\n",
    "         - 也就是将所有的P(F|C)变成上式的内容\n",
    "         \n",
    "## 2.3 API\n",
    "   + sklearn.naive_bayes.MultinomialNB(alpha = 1.0) \n",
    "        - alpha = 1.0 意即拉普拉斯平滑系数中的α\n",
    "        \n",
    "## 2.4 优缺点\n",
    "   + 发源于古典数学理论，稳定的分类效率\n",
    "   + 对缺失数据不太敏感，算法较简单，常用文本分类\n",
    "   + 分类准确度高，速度快\n",
    "  ***\n",
    "   + 样本属性独立性假设，如果样本属性有关联时效果则不好\n",
    "   \n",
    "## 2.5. 分类模型的精确率与召回率\n",
    "+ 分类模型的评估：\n",
    "    + estimator.score() 一般常见使用的准确率，预测结果正确的百分比\n",
    "    \n",
    "    ### 2.5.1 混淆矩阵\n",
    "    \n",
    "     + 分类任务下，预测结果与正确结果标记之间存在四种不同组合，构成混淆矩阵（多分类）    \n",
    "\n",
    "\n",
    " - |正例 |反例 \n",
    "      :-: | :-: | :-: \n",
    "    正例 | 真正例TP | 伪反例FN |\n",
    "    假例 | 伪反例FP | 真正例TN |\n",
    " \n",
    "      如一个二分类中，只有猫狗两个分类，对于猫这个测试类别有这样的混淆矩阵，列代表预测结果，行代表真实情况 \n",
    "  - |cat  |NO cat \n",
    "       :-: | :-: | :-: \n",
    "    cat |true positive | false negtive |\n",
    "   NO cat | false positive | true negtive |\n",
    "   \n",
    "***      \n",
    "        \n",
    "   + 召回率 = TP/（TP+FN） 得到的是预测对的占真实中对的比率\n",
    "   + 精确率 = TP/（TP+FP） 得到的预测个数中真正对的占的比率\n",
    "   + 稳健率F1-score = 2TP/(2TP+FN+FP) = 2*Precision*Recall /(Precision+Recall)\n",
    "   + API\n",
    "        + sklearn.metrics.classification_report(y_true,y_pred,target_names=None)\n",
    "            - y_true:真实目标值\n",
    "            - y_pred:关于机器预测目标值\n",
    "            - target_names:目标类别名称\n",
    "            - return 每个类别精确率与召回率\n",
    "            \n",
    "            \n",
    "## 2.6 交叉验证与网格搜索（超参数搜索）\n",
    "     \n",
    "   ### 2.6.1交叉验证\n",
    "   \n",
    "   + 为了让被评估的模型更加准确可信\n",
    "   + 如将数据集分为固定的4份（编号1，2，3，4），取1号当作验证集，其他为训练集，得到一个准确率1\n",
    "       - 取2号做验证集，其他为训练集，得到准确率2依次得到准确率3，4\n",
    "       - 求四个准确率率的平均值，该平均值为当前模型的**准确率**\n",
    "       - 其中分成4份，为**4折交叉验证**\n",
    "   + 一般与网格搜索配合使用 \n",
    "   \n",
    "### 2.6.2网格搜索\n",
    "   + 通过修改模组若干组参数进行对比(每组参数的模型都使用交叉验证求得准确率)，比较获得最优模型\n",
    "       - 如k近邻算法中，K的参数[1,2,3,4,5],那么就分别对这个参数进行交叉验证获取\n",
    "       - 一个算法有多个参数，如a：[2,4,6,8,10],b:[10,30,40]，那么a与b内部元素分别组合得到15组参数组\n",
    "       \n",
    "   + api sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)\n",
    "       - 对估计器的指定参数值进行详尽搜索\n",
    "       - estimator 估计器对象（如k近邻中实例的knn）\n",
    "       - param_grid 估计器参数（dict）{\"n_neighbors\":[1,3,5]}\n",
    "       - cv 指定进行几折交叉验证\n",
    "       ***\n",
    "       - fit 输入训练集数据\n",
    "       - score 准确率\n",
    "       ***\n",
    "       - 结果：\n",
    "       - best_score_ 交叉验证中最好的结果\n",
    "       - best_estimator_ 最好的参数模型\n",
    "       - cv_results_ 每次交叉验证后测试集准确率结果和训练集准确率结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "a = fetch_20newsgroups(subset='all')\n",
    "#print(a.target)\n",
    "x = a.data\n",
    "y = a.target\n",
    "#分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer()\n",
    "x_train = tf.fit_transform(x_train)\n",
    "x_test = tf.transform(x_test)\n",
    "#print(tf.get_feature_names())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  7  3 ... 13 15  3]\n",
      "0.8452886247877759\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.90      0.65      0.75       228\n",
      "           comp.graphics       0.87      0.74      0.80       235\n",
      " comp.os.ms-windows.misc       0.93      0.76      0.84       265\n",
      "comp.sys.ibm.pc.hardware       0.68      0.92      0.78       236\n",
      "   comp.sys.mac.hardware       0.92      0.86      0.89       243\n",
      "          comp.windows.x       0.92      0.86      0.89       254\n",
      "            misc.forsale       0.95      0.64      0.76       259\n",
      "               rec.autos       0.86      0.92      0.89       239\n",
      "         rec.motorcycles       0.95      0.95      0.95       237\n",
      "      rec.sport.baseball       0.93      0.97      0.95       240\n",
      "        rec.sport.hockey       0.94      0.98      0.96       253\n",
      "               sci.crypt       0.82      0.97      0.89       256\n",
      "         sci.electronics       0.88      0.82      0.85       228\n",
      "                 sci.med       0.97      0.93      0.95       245\n",
      "               sci.space       0.85      0.97      0.90       231\n",
      "  soc.religion.christian       0.53      0.98      0.69       245\n",
      "      talk.politics.guns       0.76      0.96      0.85       235\n",
      "   talk.politics.mideast       0.89      0.98      0.93       233\n",
      "      talk.politics.misc       0.99      0.59      0.74       200\n",
      "      talk.religion.misc       0.93      0.17      0.29       150\n",
      "\n",
      "               micro avg       0.85      0.85      0.85      4712\n",
      "               macro avg       0.87      0.83      0.83      4712\n",
      "            weighted avg       0.87      0.85      0.84      4712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mlt = MultinomialNB(alpha = 1.0) \n",
    "mlt.fit(x_train,y_train)\n",
    "y_predict = mlt.predict(x_test)\n",
    "print(y_predict)\n",
    "print(mlt.score(x_test,y_test))\n",
    "\n",
    "# 分类模型的精确率与召回率\n",
    "from sklearn.metrics import classification_report \n",
    "cv = classification_report(y_test,y_predict,target_names=a.target_names)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的文章类别为： [ 2 17  1 ... 11  4 17]\n",
      "准确率为： 0.8448641765704584\n"
     ]
    }
   ],
   "source": [
    "def naviebayes():\n",
    "    news = fetch_20newsgroups(subset='all')\n",
    "    # 进行数据分割\n",
    "    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25)\n",
    "    # 对数据集进行特征抽取\n",
    "    tf = TfidfVectorizer()\n",
    "    # 以训练集当中的词的列表进行每篇文章重要性统计['a','b','c','d']\n",
    "    x_train = tf.fit_transform(x_train)\n",
    "    #print(tf.get_feature_names())\n",
    "    x_test  = tf.transform(x_test)\n",
    "    # 进行朴素贝叶斯算法的预测\n",
    "    mlt = MultinomialNB(alpha=1.0)\n",
    "    #print(x_train.toarray())\n",
    "    mlt.fit(x_train, y_train)\n",
    "    y_predict = mlt.predict(x_test)\n",
    "    print(\"预测的文章类别为：\", y_predict)\n",
    "    # 得出准确率\n",
    "    print(\"准确率为：\", mlt.score(x_test, y_test))\n",
    "    #print(\"每个类别的精确率和召回率：\", classification_report(y_test, y_predict, target_names=news.target_names))\n",
    "    return None\n",
    "naviebayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最合适的模型准确率 0.4829787234042553\n",
      "交叉验证最好结果： 0.47012925598991173\n",
      "合适的模型： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "每个超参数每次交叉验证的结果： {'mean_fit_time': array([0.01878405, 0.01400089, 0.02198892, 0.02918868, 0.02360063]), 'std_fit_time': array([0.00719081, 0.00725732, 0.00657305, 0.00714978, 0.00695012]), 'mean_score_time': array([0.18549166, 0.19499321, 0.34744725, 0.46653771, 0.33780079]), 'std_score_time': array([0.02300317, 0.08041587, 0.08903482, 0.0630481 , 0.08994923]), 'param_n_neighbors': masked_array(data=[1, 3, 5, 7, 10],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}, {'n_neighbors': 10}], 'split0_test_score': array([0.41400304, 0.42694064, 0.44939117, 0.44710807, 0.45471842]), 'split1_test_score': array([0.41312741, 0.44092664, 0.45096525, 0.45752896, 0.45868726]), 'split2_test_score': array([0.45154963, 0.44762652, 0.47430365, 0.47861907, 0.4688113 ]), 'split3_test_score': array([0.4475693 , 0.45118521, 0.46484532, 0.4700683 , 0.47930896]), 'split4_test_score': array([0.45929276, 0.47368421, 0.48067434, 0.48766447, 0.49095395]), 'mean_test_score': array([0.43663304, 0.44766709, 0.46374527, 0.46784363, 0.47012926]), 'std_test_score': array([0.0196326 , 0.01517976, 0.01239287, 0.0145043 , 0.01326345]), 'rank_test_score': array([5, 4, 3, 2, 1]), 'split0_train_score': array([1.        , 0.66799205, 0.61520875, 0.5861829 , 0.55675944]), 'split1_train_score': array([1.        , 0.66300257, 0.61279461, 0.583878  , 0.55189146]), 'split2_train_score': array([1.        , 0.65894072, 0.60933031, 0.58220732, 0.54808166]), 'split3_train_score': array([1.        , 0.66035886, 0.61231493, 0.58005687, 0.54907344]), 'split4_train_score': array([1.        , 0.65785881, 0.6098869 , 0.57673557, 0.54816693]), 'mean_train_score': array([1.        , 0.6616306 , 0.6119071 , 0.58181213, 0.55079459]), 'std_train_score': array([0.        , 0.00361744, 0.0021249 , 0.00323667, 0.00328663])}\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索获取k近邻算法最优K模型\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./facebook_csv/train.csv')#,nrows=2000000)\n",
    "\n",
    "data = data.query('x > 1.0 & x < 1.25 &y >2.5 & y<2.75')\n",
    "#print(data)\n",
    "timevalue = pd.to_datetime(data['time'],unit='s')\n",
    "time = pd.DatetimeIndex(timevalue)\n",
    "data['day'] = time.day\n",
    "data['hour'] = time.hour\n",
    "data['weekday'] = time.weekday\n",
    "data = data.drop(['time'],axis=1)\n",
    " \n",
    "num = data.groupby(by='place_id').count()\n",
    " \n",
    "tf = num[num.row_id>3].reset_index()#将分组依据重新划分为分组后的列元素\n",
    " \n",
    "data = data[data['place_id'].isin(tf.place_id)]#保留符合条件的地点id\n",
    " \n",
    "y = data['place_id']\n",
    "x = data.drop(['place_id'],axis=1)\n",
    "\n",
    "x = x.drop(['row_id'],axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.fit_transform(x_test)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(algorithm='auto')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param ={\"n_neighbors\":[1,3,5,7,10]}\n",
    "gs = GridSearchCV(knn,param_grid=param,cv=5)\n",
    " \n",
    "#录入数据\n",
    "gs.fit(x_train,y_train)\n",
    "#得到测试结果\n",
    "#y_predict = gc.predict(x_test)\n",
    "\n",
    "\n",
    "#准确度\n",
    "print(\"最合适的模型准确率\",gs.score(x_test,y_test))\n",
    "print(\"交叉验证最好结果：\",gs.best_score_)\n",
    "print(\"合适的模型：\",gs.best_estimator_)\n",
    "print(\"每个超参数每次交叉验证的结果：\",gs.cv_results_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 决策树、随机森林\n",
    "\n",
    "*  通过对特征进行判断进行分类 \n",
    "\n",
    "## 3.1 信息论基础   \n",
    "   * 香农 信息论创始人，信息的单位：比特\n",
    "   如共32支球队，每次猜一支球队，只知道胜或败，每次选取区间都是之前的一半，只需要5次就可以获得获胜球队\n",
    "   香农对此得到的解法为：\n",
    "   > H = -(p1㏒p1+p2㏒p2+...+p32㏒p32)  \n",
    "   H 为信息熵，单位比特 \n",
    "   pi 均为 1/32，但在一定条件下，每支球队的获胜概率不一定都一样，如巴西等国家，此时的H就会小于5\n",
    "   \n",
    "  信息熵公式为：\n",
    "  >   $ H(X) = ∑_{x∈X}P(x)logP(x)  $\n",
    " \n",
    "  信息熵越大，不确定性越大\n",
    "   ### 3.1.1 信息增益\n",
    "   * 因为决策树是使用一颗类似二叉树的样子对特征进行判断分类的，所以需要通过一些手段对特征进行权重计算，区别出现后比较的特征，这一过程需要使用信息增益来判断\n",
    "   * 特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件H(D|A)之差：\n",
    "   > g(D,A) = H(D) - H(D | A)\n",
    "   \n",
    "   信息熵：\n",
    "$$\n",
    "H(D) =- \\sum_{k=1}^{k} \\frac{|C_{k}|}{|D|}log\\frac{|C_{k}|}{|D|} \n",
    "$$\n",
    "\n",
    "   条件熵：\n",
    "$$  \n",
    "H(D|A) = \\sum_{i=1}^{n} \\frac{|D_{i}|}{|D|} \\sum_{k=1}^{K}\\frac{|D_{ik}|}{|D_{i}|}log\\frac{|D_{ik}|}{|D_{i}|} \n",
    "$$\n",
    "   - $C_k$表示属于某类别的样本数。\n",
    "   - 真正计算时，条件熵又等于 P(F1)H(F1)+P(F2)H(F2)+P(F3)H(F3)+...\n",
    "       - 也即，计算这个特征值内每个分类的概率与信息熵，\n",
    "       - 如3.1.2内的年龄，则年龄下青年的概率为5/15，H(青年)=-(2/5 ㏒(2/5)+3/5 ㏒(3/5)),此处都是对“类别”列进行计算的信息熵，青年条件下，对应了5个类别值，2个为“是”\n",
    "   - 依次计算其他列对“类别”的信息增益\n",
    "\n",
    "   ### 3.1.2 案例演示\n",
    "   \n",
    "ID  | 年龄 |有工作|有房子|信贷情况|类别 \n",
    ":-: | :-:  | :-:  | :-:  | :-:    | :-:\n",
    "1   | 青年 | 否   |否    |一般    |否 |\n",
    "2   | 青年 | 否   |否    |好      |否 |\n",
    "3   | 青年 | 是   |否    |好      |是 |\n",
    "4   | 青年 | 是   |是    |一般    |是 |\n",
    "5   | 青年 | 否   |否    |一般    |否 |\n",
    "6   | 中年 | 否   |否    |一般    |否 |\n",
    "7   | 中年 | 否   |否    |好      |否 |\n",
    "8   | 中年 | 是   |是    |好      |是 |\n",
    "9   | 中年 | 否   |是    |非常好  |是 |\n",
    "10  | 中年 | 否   |是    |非常好  |是 |\n",
    "11  | 老年 | 否   |是    |非常好  |是 |\n",
    "12  | 老年 | 否   |是    |好      |是 |\n",
    "13  | 老年 | 是   |否    | 好     |是 |\n",
    "14  | 老年 | 是   |否    |非常好  |是 |\n",
    "15  | 老年 | 否   |否   |一般     |否 |\n",
    "\n",
    "\n",
    "***\n",
    "           \n",
    "   * 1    因为此案例是判断其他列值，然后将结果分类为“类别”列的值内，最后一列的类别就是分类结果，因此对于“类别”列的信息值为\n",
    "           H(类别) = -(9/15))㏒(9/15)+(6/15))㏒(6/15) ≈0.9\n",
    "   * 2    然后计算出ID，类别列外的其他列的信息增益值，即计算g(类别，年龄),g(类别，有工作),g(类别，有房子),g(类别，信贷情况)\n",
    "       + H(类别|年龄) = P(青年)H(青年)+P(中年)H(中年)+P(老年)H(老年)\n",
    "                   = 5/15 * (3/5㏒(3/5)+2/5㏒(2/5) )+ 5/15 * (2/5㏒(2/5)+3/5㏒(3/5) ) + 5/15 * (4/5㏒(4/5)+1/5㏒(1/5) )\n",
    "   * 3    g(类别，年龄) = H(类别) - H(类别|年龄)\n",
    "       + 依次计算出 g(类别，年龄),g(类别，有工作),g(类别，有房子),g(类别，信贷情况)\n",
    "   * 4 比较这四个信息增益，这个大小顺序即为决策树的分类依据\n",
    "***  \n",
    "## 3.2 分类依据使用算法\n",
    "- ID3 信息增益最大准则\n",
    "- C4.5 信息增益比最大准则\n",
    "- **CART**  划分更加仔细，多在sklearn中使用\n",
    "   - 回归树：平方误差最小\n",
    "   - 分类树：基尼系数最小准则，在sklearn中可以选择划分原则\n",
    "       \n",
    "## 3.3 API\n",
    "- sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)\n",
    "    - 决策树分类器\n",
    "    - criterion 默认是‘gini’系数，可以修改为信息增益熵‘entropy’\n",
    "    - max_depth 树的深度\n",
    "    - random——state 随机数种子\n",
    "    ***\n",
    "    - method：\n",
    "        - decision_path：返回决策树路径\n",
    "        \n",
    "## 3.4 案例\n",
    "- [泰坦尼克号数据](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt)\n",
    "    - 列名\"row.names\",\"pclass\",\"survived\",\"name\",\"age\",\"embarked\",\"home.dest\",\"room\",\"ticket\",\"boat\",\"sex\"\n",
    "    - 其中\"pclass\",\"survived\",\"age\",\"embarked\",\"home.dest\",\"room\",\"ticket\",\"boat\",\"sex\"是有影响的特征值，age列有缺失\n",
    "    - \n",
    "## 3.5 决策树的结构，本地保存\n",
    "- sklearn.tree.export_graphviz() 该函数能够导出DOT格式\n",
    "    - tree.export_graphviz(etimator,out_file='tree.dot',feature_names=['',''])\n",
    "    - 安装能够将dot转换为pdf，png的软件\n",
    "    \n",
    "    - 下载安装graphviz，将bin目录添加进path，命令找到导出的dot文件，然后：即可得到对应的png图片\n",
    "        - dot -Tpng tree.dot -o sample.png\n",
    "        \n",
    "## 3.6 优缺点与改进\n",
    "+ 理解简单与解释，数目可视化\n",
    "+ 数据准备少，基本不需要归一化数据\n",
    "***\n",
    "+ 决策树学习者可创建不能很好的地推广数据的过于复杂的树，--过拟合\n",
    "***\n",
    "+ 减枝cart算法（sklearn实现，与随机森林，参数调优有关）\n",
    "+ 随机森林\n",
    "\n",
    "## 3.7 集成学习方法\n",
    "+ 通过建立多个模型组合来解决单一预测问题\n",
    "+ 原理：生成多个分类器/模型，各自独立学习和做出预测，最后结合成单预测，因此优于任何一个单分类的做出预测\n",
    "\n",
    "## 3.8 随机森林\n",
    "+ 机器学习中，包含多个决策树的分类器，并输出的类别是由个别树输出的类别的众数而定\n",
    "    - 单个树建立：\n",
    "        - 随机在N个样本当中选择一个样本，重复N次（样本有可能重复，）\n",
    "        - 随机在M个特征当中选出m个特征，m（n<m）取值\n",
    "        - 此时的决策树使用的数据集基本上来说就是一个特殊的数据集，数据量为N，特征m个\n",
    "        \n",
    "    - 建立10棵决策树，样本，特征也是不一样的\n",
    "    \n",
    "      ***\n",
    "    - 随即森林是随机而有放回的抽样\n",
    "       - 随机这样每棵树的训练集都是不同的，训练出的分类结果也是不同的\n",
    "       - 如果不放回的抽样，每棵树的训练样本均不同， 且无交际，每棵树都是有偏的，片面的\n",
    "       - 随机森林的分类结果取决于多棵树（弱分类器）的投票表决\n",
    "+ api\n",
    "    - sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None)\n",
    "        - 随机森林分类器\n",
    "        - n_estimators=10，integer，optional（defalut=10）森林里的数目数量，120，200，300，500，800，1200\n",
    "        - criterion='gini' string 可选，默认gini，分割特征的测量方法\n",
    "        - max_depth=None integer或None，可选默认为无，树的最大深度\n",
    "        - bootstrap=True boolean,可选默认为True，是否在构建树时使用放回抽样 \n",
    "        - random_state=None\n",
    "        - max_features = 'auto' 每个决策树的最大特征数量\n",
    "            - auto 为n_features的开方\n",
    "            - sqrt 与auto一致\n",
    "            - log2 log2（n_features）\n",
    "            - None 为n_features\n",
    "+ 优缺点\n",
    "    - 当前算法中，具有极好准确率\n",
    "    - 能有效运行在大数据集上\n",
    "    - 处理具有高维特征的输入样本，而且不需要降维\n",
    "    - 能够评估各个特征在分类问题上的重要性\n",
    "+ 案例，采用网格搜索的方式，来查询更优的超参数\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row.names pclass  survived  \\\n",
      "0          1    1st         1   \n",
      "1          2    1st         0   \n",
      "2          3    1st         0   \n",
      "3          4    1st         0   \n",
      "4          5    1st         1   \n",
      "\n",
      "                                              name      age     embarked  \\\n",
      "0                     Allen, Miss Elisabeth Walton  29.0000  Southampton   \n",
      "1                      Allison, Miss Helen Loraine   2.0000  Southampton   \n",
      "2              Allison, Mr Hudson Joshua Creighton  30.0000  Southampton   \n",
      "3  Allison, Mrs Hudson J.C. (Bessie Waldo Daniels)  25.0000  Southampton   \n",
      "4                    Allison, Master Hudson Trevor   0.9167  Southampton   \n",
      "\n",
      "                         home.dest room      ticket   boat     sex  \n",
      "0                     St Louis, MO  B-5  24160 L221      2  female  \n",
      "1  Montreal, PQ / Chesterville, ON  C26         NaN    NaN  female  \n",
      "2  Montreal, PQ / Chesterville, ON  C26         NaN  (135)    male  \n",
      "3  Montreal, PQ / Chesterville, ON  C26         NaN    NaN  female  \n",
      "4  Montreal, PQ / Chesterville, ON  C22         NaN     11    male  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1313 entries, 0 to 1312\n",
      "Data columns (total 11 columns):\n",
      "row.names    1313 non-null int64\n",
      "pclass       1313 non-null object\n",
      "survived     1313 non-null int64\n",
      "name         1313 non-null object\n",
      "age          633 non-null float64\n",
      "embarked     821 non-null object\n",
      "home.dest    754 non-null object\n",
      "room         77 non-null object\n",
      "ticket       69 non-null object\n",
      "boat         347 non-null object\n",
      "sex          1313 non-null object\n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 112.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titan = pd.read_csv(\"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt\")\n",
    "print(titan.head(5))\n",
    "print(titan.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']\n",
      "[[31.19418104  0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  0.          1.          0.          1.          0.        ]\n",
      " ...\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  0.          0.          1.          1.          0.        ]\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]]\n",
      "[[32.          0.          1.          0.          0.          1.        ]\n",
      " [64.          1.          0.          0.          0.          1.        ]\n",
      " [56.          1.          0.          0.          1.          0.        ]\n",
      " ...\n",
      " [26.          0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  1.          0.          0.          0.          1.        ]\n",
      " [18.          0.          0.          1.          0.          1.        ]]\n",
      "0.7872340425531915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\pandas\\core\\generic.py:6130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "x = titan[['pclass','age','sex']]\n",
    "y = titan[['survived']]\n",
    "x['age'].fillna(x['age'].mean(),inplace=True)\n",
    "#print(x)\n",
    "#分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "# one hot 处理\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dict = DictVectorizer(sparse=False)\n",
    "x_train = dict.fit_transform(x_train.to_dict(orient='records'))# to_dict将数组转换为字典\n",
    "print(dict.get_feature_names())\n",
    "print(x_train)\n",
    "x_test = dict.transform(x_test.to_dict(orient='records'))\n",
    "print(x_test)\n",
    "\n",
    "from  sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "dec = DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)\n",
    "dec.fit(x_train,y_train)\n",
    "print(dec.score(x_test,y_test))\n",
    "\n",
    "export_graphviz(dec,out_file='tree.dot',feature_names=dict.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\pandas\\core\\generic.py:6130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']\n",
      "准确率： 0.8206686930091185\n",
      "查看选择的参数模型： {'max_depth': 5, 'n_estimators': 300}\n",
      "[0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0\n",
      " 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
      " 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1]\n",
      "[0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
      " 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1\n",
      " 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1]\n",
      "6.076510429382324\n"
     ]
    }
   ],
   "source": [
    "# 随机森林\n",
    "import time\n",
    "import time\n",
    "a = time.time()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "x = titan[['pclass','age','sex']]\n",
    "y = titan[['survived']]\n",
    "x['age'].fillna(x['age'].mean(),inplace=True)\n",
    "#print(x)\n",
    "#分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "# one hot 处理\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dict = DictVectorizer(sparse=False)\n",
    "x_train = dict.fit_transform(x_train.to_dict(orient='records'))# to_dict将数组转换为字典\n",
    "print(dict.get_feature_names())\n",
    "\n",
    "\n",
    "x_test = dict.transform(x_test.to_dict(orient='records'))\n",
    "#print(x_test)\n",
    "\n",
    "rf = RandomForestClassifier( )\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param ={'n_estimators':[100,120,300],'max_depth':[5,8,15,25,30] }\n",
    "gs = GridSearchCV(rf,param_grid=param,cv=2)\n",
    "gs.fit(x_train,y_train.values.ravel())\n",
    "print(\"准确率：\",gs.score(x_test,y_test)) \n",
    "print(\"查看选择的参数模型：\",gs.best_params_) \n",
    "\n",
    "#print(y_test['survived'].values)\n",
    "#print(gs.predict(x_test))\n",
    "print(time.time()-a)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# export_graphviz(dec,out_file='tree.dot',feature_names=dict.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.000299453735352\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = time.time()\n",
    "time.sleep(10)\n",
    "print(time.time()-a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n",
      "329\n",
      "270\n",
      "0.8206686930091185\n"
     ]
    }
   ],
   "source": [
    "true_ = y_test['survived'].values\n",
    "pre = gs.predict(x_test)\n",
    "print(len(true_))\n",
    "print(len(pre))\n",
    "\n",
    "x =[ 1 if true_[i]==pre[i] else 0 for i in range(len(true_))]\n",
    "print(sum(x))\n",
    "print(float(sum(x))/len(true_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四部分 针对连续性数据\n",
    "***\n",
    "# 1. 线性回归\n",
    "+ 寻找一种能预测的趋势\n",
    "\n",
    "## 1.1 定义\n",
    "+ 通过一个或多个自变量与因变量之间进行建模的回归分析\n",
    "    - 一元线性回归：涉及到变量只有一个\n",
    "        - 二维：直线关系\n",
    "        - y = kx + b \n",
    "        - x 可以是多个特征，k可以是多个关系，b同样可以是多个偏移量\n",
    "        \n",
    "    - 多元线性回归：涉及到变量两个或以上\n",
    "        - 三位：特征，目标，平面关系\n",
    "        \n",
    "## 1.2 通用公式：\n",
    "$$\n",
    "        h(w) = w_0+w_1x_1+w_2x_2+w_3x_3...+w_dx_d \\\\  \n",
    "    = w^Tx \n",
    "    $$      \n",
    "   其中\n",
    "    $$\n",
    "    w =\\begin{pmatrix}w_0\n",
    "    \\\\ w_1\n",
    "    \\\\ w_2\n",
    "    \\end{pmatrix}\n",
    "    ,x = \\begin{pmatrix}1\n",
    "    \\\\ x_1\n",
    "    \\\\ x_2\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    " ### 1.2.1 矩阵的乘法：\n",
    "   - (m,n) * (n,p)，第一个矩阵的列与第二个矩阵的行相同才会有乘法的定义\n",
    "   - 第一矩阵的行分别与第二矩阵的列对应相乘累加，1行1列为新矩阵的（1，1）元素，然后1行2列为（1，2）元素，依次类推\n",
    "   - 最终得到的是（m,p）的矩阵\n",
    "    \n",
    "## 1.3 损失函数\n",
    "   - 预测时有误差，**计算训练集的每个样本的预测值与实际值之差的平分的累加和**\n",
    "   - 总损失函数定义：\n",
    "        $$  \n",
    "        J(θ)=(h_w(x_1)-y_1)^2+(h_w(x_2)-y_2)^2+(h_w(x_3)-y_3)^2+...+(h_w(x_m)-y_m)^2  \\\\ = \\sum_{i=1}^{m}(h_w(x_i)-y_i)^2  \\\\   \n",
    "        y_i 为第i个训练样本的真实值 \\\\ h_w(x_i)为第i个训练样本与特征值组合预测函数（即，特征与目标的关系函数） \n",
    "    $$  \n",
    "    又称**最小二乘法**\n",
    "\n",
    "## 1.4 优化\n",
    " - 因为计算损失函数，可以对线性回归中的权重w进行优化,\n",
    " - 数据量过大时，采用SGD方式（梯度下降）更快\n",
    " \n",
    "  ### 1.4.1 最小二乘法的正规方程：\n",
    "     $$\n",
    "         w = (X^TX)^{-1}X^Ty \\\\ X为特征值矩阵， y为目标值矩阵 \n",
    "     $$\n",
    " 缺点：特征过于复杂时，求解速度太慢\n",
    " \n",
    "  ### 1.4.2最小二乘法的梯度下降：\n",
    "     $$\n",
    "     w1: = -w1-α\\frac{ \\partial cost(w0+w1x1))}{\\partial w1}\\\\\n",
    "      w0: = -w0-α\\frac{ \\partial cost(w0+w1x1))}{\\partial w1}\\\\\n",
    "      α为学习速率，需要手动指定;\n",
    "      \\frac{ \\partial cost(w0+w1x1))}{\\partial w1} 表示方向\\\\沿着这个函数下降的方向找，最终寻找到谷底，然后更新w值\n",
    "     $$\n",
    "     \n",
    " ### 1.4.3 优化API\n",
    " - sklearn.linear_model.LinerRegression 正规方程\n",
    "     - coef_:回归系数,即所求的w\n",
    " - sklearn.linear_model.SGDRegressor 梯度下降\n",
    "     - coef_:回归系数，所求的w\n",
    "     \n",
    " ### 1.4.4 两种优化对比\n",
    " \n",
    "|梯度下降|正规方程|\n",
    "| ------ | ------ |\n",
    "|需要对此迭代|一次运算得出|\n",
    "|需要选择学习率α|不需要|\n",
    "|当特征数量n大时也能较好适用|需要计算$(X^TX)^{-1}$，<br>如果特征数量n较大，逆矩阵计算时间复杂度为$O(n^3)$ ，通常来说当n<10000时可以接受|\n",
    "|使用各种类型的模型|只适用于线性模型，不适合逻辑回归模型等其他模型|\n",
    "\n",
    "\n",
    "## 1.5 回归性能评估\n",
    "   ### 1.5.1 均方误差评价机制(Mean Squared Error MSE)：\n",
    "     \n",
    "$$\n",
    "     MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y^i - \\frac{ }{y})^2 \\\\\n",
    "     y^i 为预测值；\\frac{ }{y} 为真实值\n",
    "     $$\n",
    "   ### 1.5.2 API\n",
    " sklearn.metrics.mean_squared_error(y_true,y_pred)\n",
    " + 均方误差回归损失\n",
    " + y_true:真实值 标准化之前的值\n",
    " + y_pred:预测值 标准化之前的值\n",
    " + return :浮点数结果\n",
    " \n",
    "## 1.6 过拟合与欠拟合\n",
    "   + 正常来来说，一个模型，可能最好的情况是一个一元二次函数的样子-》f(x)=w0+w1x+w2x^2\n",
    "   \n",
    "   + 欠拟合： 学习区分标准少，特征少,拟合成f(x)=w0+w1x\n",
    "       - 在训练数据上不能获得更好的拟合，同时在训练集以外的数据集上也不能很好的拟合数据，此时该假设出现了欠拟合的现象（模型过于简单）\n",
    "       - 需要增加特征数量\n",
    "   + 过拟合： 学习特征多，但是存在部分偏差的信息 拟合出f(x)=w0+w1x+w2x^2+w3x^3,\n",
    "       - 在训练数据熵获得比其他假设更好的你好，但是在训练集以外的数据集上却不能很好的拟合数据，此时认为出现了过拟合（模型过于复杂）\n",
    "       - 进行特征选择，消除关联性大的特征（难实现）\n",
    "       - **交叉验证**\n",
    "       - 正则化 \n",
    "          - 使得w的每个元素都很小，接近于0，\n",
    "          - 优点，越小的参数，说明模型越简单，越简单的模型越不容易产生过拟合\n",
    "## 1.7 岭回归\n",
    "+ 为解决过拟合的情况，岭回归带有对特征的正则化操作\n",
    "    - L2正则化\n",
    "    \n",
    "### 1.7.1 api\n",
    "sklearn.linear_model.Ridge(alpha=1.0)\n",
    " - 具有L2正则化的线性最小二乘法\n",
    " - alpha 正则化力度 一般在0~1之间取float，数据再大可以再1~10内取float。（ $10^{-2}$  ~$10^{-10}$,值越大，则w越接近0）\n",
    " ***\n",
    " - coef_ \n",
    " \n",
    "# 1.8 训练模型保存与加载\n",
    "from sklearn.externals import joblib\n",
    "   - lq = joblib.dump(knn,'./test.pkl')\n",
    "   - lq = joblib.load('./test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:06:21.082170Z",
     "start_time": "2019-04-03T05:06:21.072177Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6  8]\n",
      " [10 12 14 16]\n",
      " [18 20 22 24]]\n"
     ]
    }
   ],
   "source": [
    "# 矩阵运算\n",
    "import numpy as np \n",
    "a = [[1,2,3,4],[5,6,7,8],[9,10,11,12]]\n",
    "b = [[2],[2],[2]]\n",
    " \n",
    "\n",
    "print(np.multiply(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T06:57:15.276440Z",
     "start_time": "2019-04-03T06:57:15.217470Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.78759946]\n",
      " [21.98127624]\n",
      " [ 6.24082397]\n",
      " [29.34107084]\n",
      " [20.57936519]\n",
      " [33.54730738]\n",
      " [16.28376951]\n",
      " [28.75167142]\n",
      " [13.71360383]\n",
      " [19.19683563]\n",
      " [12.96876074]\n",
      " [24.88921142]\n",
      " [18.02049444]\n",
      " [20.94795527]\n",
      " [15.50858516]\n",
      " [14.69669252]\n",
      " [12.36278738]\n",
      " [27.8139339 ]\n",
      " [29.31984388]\n",
      " [19.43144769]\n",
      " [18.63623667]\n",
      " [13.57710573]\n",
      " [45.04515694]\n",
      " [14.903103  ]\n",
      " [17.14672516]\n",
      " [35.64963552]\n",
      " [11.95043594]\n",
      " [10.83563343]\n",
      " [25.58632745]\n",
      " [20.39922236]\n",
      " [14.17949802]\n",
      " [23.13404614]\n",
      " [26.15449484]\n",
      " [20.24220675]\n",
      " [19.3216665 ]\n",
      " [19.43091742]\n",
      " [33.84258135]\n",
      " [25.53808494]\n",
      " [25.95408784]\n",
      " [26.03620661]\n",
      " [17.58202832]\n",
      " [11.70718821]\n",
      " [31.69037447]\n",
      " [20.26587293]\n",
      " [15.91461148]\n",
      " [23.82720084]\n",
      " [22.62936678]\n",
      " [20.5291135 ]\n",
      " [15.41573813]\n",
      " [ 9.97088479]\n",
      " [32.49475891]\n",
      " [17.44034671]\n",
      " [25.58625517]\n",
      " [27.75322952]\n",
      " [13.36764227]\n",
      " [20.76740345]\n",
      " [16.7932001 ]\n",
      " [ 6.3326952 ]\n",
      " [42.73295784]\n",
      " [31.18749909]\n",
      " [24.94634265]\n",
      " [23.65508613]\n",
      " [17.91718968]\n",
      " [25.52047554]\n",
      " [24.15797778]\n",
      " [20.13115753]\n",
      " [38.17753579]\n",
      " [24.69197788]\n",
      " [41.53021689]\n",
      " [33.61608028]\n",
      " [40.12515522]\n",
      " [12.48487846]\n",
      " [12.95291234]\n",
      " [ 2.53354744]\n",
      " [35.46621544]\n",
      " [34.74332527]\n",
      " [29.76602039]\n",
      " [25.47636643]\n",
      " [21.74878667]\n",
      " [22.95190753]\n",
      " [27.4840871 ]\n",
      " [18.11338591]\n",
      " [25.18212688]\n",
      " [17.86334808]\n",
      " [28.74419498]\n",
      " [16.37717652]\n",
      " [36.34921766]\n",
      " [22.40317238]\n",
      " [24.9259659 ]\n",
      " [16.07538289]\n",
      " [21.9511707 ]\n",
      " [35.13801505]\n",
      " [14.50636021]\n",
      " [17.28247366]\n",
      " [12.01091291]\n",
      " [32.63783335]\n",
      " [17.92395524]\n",
      " [28.32909698]\n",
      " [22.64299466]\n",
      " [10.45232954]\n",
      " [10.32841504]\n",
      " [28.66141558]\n",
      " [16.11458469]\n",
      " [32.74919757]\n",
      " [ 9.36806508]\n",
      " [24.77170541]\n",
      " [22.83427067]\n",
      " [24.10732497]\n",
      " [30.12788967]\n",
      " [23.05700607]\n",
      " [23.9138581 ]\n",
      " [32.6049617 ]\n",
      " [14.37106766]\n",
      " [15.98190156]\n",
      " [21.14790908]\n",
      " [33.14188588]\n",
      " [12.10414454]\n",
      " [15.62367306]\n",
      " [23.73273349]\n",
      " [24.60844784]\n",
      " [27.94018216]\n",
      " [36.12803883]\n",
      " [35.35332313]\n",
      " [22.49275573]\n",
      " [21.89102571]\n",
      " [23.74457277]\n",
      " [29.38982257]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def myliner():\n",
    "    # 获取数据\n",
    "    lb = load_boston()\n",
    "    # 分割数据\n",
    "    # print(lb.DESCR)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        lb.data, lb.target, test_size=0.25)\n",
    "    # print(y_train)\n",
    "    # 进行标准化,对特征值与目标值都进行转化\n",
    "    # 实例化两个标准化api\n",
    "    std_x = StandardScaler()\n",
    "    x_train = std_x.fit_transform(x_train)\n",
    "    x_test = std_x.transform(x_test)\n",
    "\n",
    "    # 目标值标准化\n",
    "    std_y = StandardScaler()\n",
    "    #\n",
    "    y_train = std_y.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test = std_y.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # 线性回归预测\n",
    "    li = LinearRegression()\n",
    "    li.fit(x_train, y_train)\n",
    "    print('权重系数：', li.coef_)\n",
    "\n",
    "    tar = std_y.inverse_transform(li.predict(x_test)).reshape((1, -1))[0]\n",
    "    tars = li.predict(x_test).reshape((1, -1))[0]\n",
    "    #print(tars)\n",
    "    # print('预测值：',tar)\n",
    "    tru = std_y.inverse_transform(y_test).reshape((1, -1))[0]\n",
    "    trus = y_test.reshape((1, -1))[0]\n",
    "    #print(trus)\n",
    "    # print('真实值：',tru)\n",
    "    print(len(tar), len(tru))\n",
    "    print('准确度', li.score(x_test, y_test))\n",
    "\n",
    "    s = [1 if abs(abs(round(tar[i], 6)) - abs(round(tru[i], 6)))\n",
    "         < 5 else 0 for i in range(len(trus))]\n",
    "    #print(sum(s))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def mySGDliner():\n",
    "    # 获取数据\n",
    "    lb = load_boston()\n",
    "    # 分割数据\n",
    "    # print(lb.DESCR)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        lb.data, lb.target, test_size=0.25)\n",
    "    # print(y_train)\n",
    "    # 进行标准化,对特征值与目标值都进行转化\n",
    "    # 实例化两个标准化api\n",
    "    std_x = StandardScaler()\n",
    "    x_train = std_x.fit_transform(x_train)\n",
    "    x_test = std_x.transform(x_test)\n",
    "\n",
    "    # 目标值标准化\n",
    "    std_y = StandardScaler()\n",
    "    #\n",
    "    y_train = std_y.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test = std_y.transform(y_test.reshape(-1, 1))\n",
    "# 线性回归预测,梯度方式\n",
    "    li = SGDRegressor()\n",
    "    li.fit(x_train, y_train)\n",
    "    print('SGD权重系数：', li.coef_)\n",
    "\n",
    "    tar = std_y.inverse_transform(li.predict(x_test)).reshape((1, -1))[0]\n",
    "    tars = li.predict(x_test).reshape((1, -1))[0]\n",
    "    #print(tars)\n",
    "    # print('预测值：',tar)\n",
    "    tru = std_y.inverse_transform(y_test).reshape((1, -1))[0]\n",
    "    trus = y_test.reshape((1, -1))[0]\n",
    "    #print(trus)\n",
    "    # print('真实值：',tru)\n",
    "    #print(len(tar), len(tru))\n",
    "    print('SGD准确度', li.score(x_test, y_test))\n",
    "\n",
    "    s = [1 if abs(abs(round(tars[i], 6)) - abs(round(trus[i], 6)))\n",
    "         < 0.4 else 0 for i in range(len(trus))]\n",
    "    #print(sum(s))\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    ms = mean_squared_error(tru, tar)\n",
    "    print(\"SGD误差值\", ms)\n",
    "\n",
    "    return None\n",
    "\n",
    "def myRDliner():\n",
    "    # 获取数据\n",
    "    lb = load_boston()\n",
    "    # 分割数据\n",
    "    # print(lb.DESCR)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        lb.data, lb.target, test_size=0.25)\n",
    "    # print(y_train)\n",
    "    # 进行标准化,对特征值与目标值都进行转化\n",
    "    # 实例化两个标准化api\n",
    "    std_x = StandardScaler()\n",
    "    x_train = std_x.fit_transform(x_train)\n",
    "    x_test = std_x.transform(x_test)\n",
    "\n",
    "    # 目标值标准化\n",
    "    std_y = StandardScaler()\n",
    " \n",
    "    y_train = std_y.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test = std_y.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "# 岭回归回归预测\n",
    "    from sklearn.linear_model import Ridge \n",
    "    li = Ridge(alpha = 1.0)\n",
    "    li.fit(x_train, y_train)\n",
    "    print('RD权重系数：', li.coef_)\n",
    "\n",
    "    tar = std_y.inverse_transform(li.predict(x_test)) \n",
    "     \n",
    "    tru = std_y.inverse_transform(y_test) \n",
    " \n",
    "    print('RD准确度', li.score(x_test, y_test))\n",
    "\n",
    "    \n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    ms = mean_squared_error(tru, tar)\n",
    "    print(\"RD误差值\", ms)\n",
    "\n",
    "    b#保存训练模型\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    joblib.dump(li,'./test.pkl')\n",
    "    return None\n",
    "\n",
    "def myload():\n",
    "    # 获取数据\n",
    "    lb = load_boston()\n",
    "    # 分割数据\n",
    "    # print(lb.DESCR)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        lb.data, lb.target, test_size=0.25)\n",
    "    # print(y_train)\n",
    "    # 进行标准化,对特征值与目标值都进行转化\n",
    "    # 实例化两个标准化api\n",
    "    std_x = StandardScaler()\n",
    "    x_train = std_x.fit_transform(x_train)\n",
    "    x_test = std_x.transform(x_test)\n",
    "\n",
    "    # 目标值标准化\n",
    "    std_y = StandardScaler()\n",
    " \n",
    "    y_train = std_y.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test = std_y.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "# 岭回归回归预测\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    lq = joblib.load('./test.pkl')\n",
    "    tar = std_y.inverse_transform(lq.predict(x_test)) \n",
    "    tru = std_y.inverse_transform(y_test) \n",
    "    \n",
    "    \n",
    "    print(std_y.inverse_transform(lq.predict(x_test)))\n",
    "\n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "#myliner()\n",
    "#mySGDliner()\n",
    "#myRDliner()\n",
    "myload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T06:35:54.014014Z",
     "start_time": "2019-04-03T06:35:54.010015Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression,SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.逻辑回归\n",
    "+ 将回归拟合编程一个分类的模型\n",
    "+ 该模型的输入是线性回归的模型\n",
    "+ 是一个二分类问题模型\n",
    "## 2.1  逻辑回归公式与Sigmoid函数\n",
    "   -\n",
    "$$\n",
    "h_θ(x)=g(θ^Tx)= \\frac {1}{1+e^{-θ^Tx}}  \\\\输出：[0，1]区间的概率值，默认0.5为阈值 \\\\ g(z) =\\frac {1}{1+e^{-z}}\\\\g(z)为sigmoid函数\n",
    "$$\n",
    "   ***\n",
    "## 2.2 损失函数\n",
    "+ 与线性回归原理相同，因为是分类问题，损失函数不同，只能通过梯度下降求解\n",
    "+ 对数似然损失函数：\n",
    "    $$\n",
    "    cost(h_θ(x),y) = \\begin{cases}\n",
    "-log(h_θ(x)) & \\text{ if } x=1 \\\\ \n",
    "-log(1-h_θ(x)) & \\text{ if } x=0 \n",
    "\\end{cases}\n",
    "    $$\n",
    "    - 存在多个最小值，因此出现-多个局部最小值\n",
    "    - 该问题无法解决，即求解最小值，只能近似就取\n",
    "        - 多次随机初始化，多次比较求一个小值结果\n",
    "        - 调整学习率 \n",
    "    - sklearn主要通过调整学习率实现的\n",
    "+ 完整损失函数：\n",
    "    $$\n",
    "    cost(h_θ(x),y)=\\sum_{i=1}^{m}-y_ilog(h_θ(x)) -(1-y_i)log(1-h_θ(x))\\\\ cost损失值越小，预测类别准确度就越高，y_i为类别值\n",
    "    $$\n",
    "    \n",
    "## 2.3 API\n",
    "+ 同样存在过拟合问题，但是自带了正则化\n",
    "+ sklearn.linear_model.LogisticRegression(penalty='l2',C=1.0)\n",
    "    - penalty ,正则化\n",
    "    - C= 1.0 正则化力度\n",
    " ***\n",
    "    - coef_：回归系数\n",
    "    \n",
    "## 2.4 案例\n",
    "### 2.4.1 [预测癌症](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)\n",
    "+ 数据描述\n",
    "    - 699条样本，共11列数据，第一列为检索id，后9列为医学特征，最后一列为癌症类型的数值\n",
    "    - 包含16个缺失值，使用？标出\n",
    "+ 选择判断类别（即选择一个类别为正例）\n",
    "    - 逻辑回归只针对正例的概率来预测分类，即，当阈值为0.5时，正例概率为0.51样本就是正例分类，如0.49则为反例分类\n",
    "    - 一般选择概率小的一方为正例（即占数据集中的类别较少的一类）\n",
    "    \n",
    "### 2.4.2 [广告点击](https://www.kaggle.com/c/avito-context-ad-clicks/data)\n",
    "\n",
    "## 2.5 逻辑回归与朴素贝叶斯的比较\n",
    "|-|逻辑回归|朴素贝叶斯|\n",
    "|------|-------|------|\n",
    "|解决问题|二分类|多分类|\n",
    "|应用场景|二分类（需要改率场景）|文本分类（主要）|\n",
    "|参数|正则化，力度|无|\n",
    "|-|得到的结果都有概率解释||\n",
    "\n",
    "## 2.6 判别模型与生成模型\n",
    "+ 判别模型：k近邻，决策树，随机森林，神经网络，逻辑回归\n",
    "+ 生成模型：需要从历史数据中总结出先验概率\n",
    "    - 朴素贝叶斯，隐马尔可夫模型，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T12:30:04.560064Z",
     "start_time": "2019-04-06T12:30:04.300199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid函数\n",
    "import mpl_toolkits.axisartist as axisartist\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "#使用axisartist.Subplot方法创建一个绘图区对象ax\n",
    "ax = axisartist.Subplot(fig, 111)  \n",
    "fig.add_axes(ax)\n",
    "ax.axis[:].set_visible(False)\n",
    "ax.axis[\"x\"] = ax.new_floating_axis(0,0)\n",
    "ax.axis[\"x\"].set_axisline_style(\"->\", size = 1.0)\n",
    "ax.axis[\"y\"] = ax.new_floating_axis(1,0)\n",
    "ax.axis[\"y\"].set_axisline_style(\"-|>\", size = 1.0)\n",
    "ax.axis[\"x\"].set_axis_direction(\"top\")\n",
    "ax.axis[\"y\"].set_axis_direction(\"right\")\n",
    "x = np.arange(-15,15,0.1)\n",
    "y=1/(1+np.exp(-x))\n",
    "plt.xlim(-10,10)\n",
    "plt.ylim(-1, 1)\n",
    "plt.plot(x,y, c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-06T13:25:57.574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
      "0             1000025                5                        1   \n",
      "\n",
      "   Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
      "0                         1                  1                            2   \n",
      "\n",
      "  Bare Nuclei  Bland Chromatin  Normal Nuucleoli  Mitoses  Class  \n",
      "0           1                3                 1        1      2  \n",
      "0.9824561403508771\n",
      "系数： [[1.43027173 0.24119816 0.80998426 0.62329575 0.44690422 1.19276443\n",
      "  0.86483851 0.45232871]]\n",
      "召回率               precision    recall  f1-score   support\n",
      "\n",
      "          良心       0.98      0.99      0.99       110\n",
      "          恶性       0.98      0.97      0.98        61\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       171\n",
      "   macro avg       0.98      0.98      0.98       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\ipykernel_launcher.py:20: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "c:\\d_files\\miniconda\\envs\\cpython37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = \"./cancer_csv/breast-cancer-wisconsin.data\"\n",
    "column = ['Sample code number','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nuucleoli','Mitoses','Class']\n",
    "data = pd.read_csv(path,names=column)\n",
    "# print(len(column))\n",
    "print(data.head(1))\n",
    "\n",
    "# 缺失值处理\n",
    "import numpy as np\n",
    "data = data.replace(to_replace=\"?\",value=np.nan)\n",
    "data = data.dropna()\n",
    "\n",
    "#分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test,y_train,y_test = train_test_split(data[column[1:-2]],data[column[-1]],test_size=0.25)\n",
    "#标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.transform(x_test)\n",
    "\n",
    "#逻辑回归预测\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lo = LogisticRegression(penalty='l2',C=1.0)\n",
    "lo.fit(x_train,y_train)\n",
    "print(lo.score(x_test,y_test))\n",
    "print(\"系数：\",lo.coef_)\n",
    "y_predict = lo.predict(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print('召回率',classification_report(y_test,y_predict,labels=[2,4],target_names=[\"良心\",\"恶性\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五部分 针对非监督学习\n",
    "# 1. k-means\n",
    "+ 非监督学习：只有特征值，无目标值，根据当前特征，将数据划分到一些类别内\n",
    "+ 存在超参数，K，将数据划分为K个类别\n",
    "## 1.1聚类过程：\n",
    "+ 0.假设有1000样本数据，可认为这是1000个分布在一个平面内的点\n",
    "+ 1. 随机从数据中抽取k=3 个样本，当作3个类别的中心点\n",
    "+ 2.计算其余点到这三个类别中心点的距离(每个样本有三个距离值)\n",
    "  + 2.2.每个样本从自己的三个距离中选择最近距离的中心点作为自己的**标记**，形成三个族群\n",
    "+ 3.计算三个族群的平均值，如只有两个特征时，每个点均为(x,y)，那么就计算每个族群内点的(x的平均值,y平均值)，得到的平均值与原有三个旧中心点进行比较，如果相同，则结束聚类，否则将三个平均值当作新的中心点，重复第二步\n",
    "\n",
    "## 1.2 API\n",
    "+ sklearn.cluster.KMeans(n_cluster=8,init='k-means++')\n",
    "    - k-measn 聚类\n",
    "    - n_clusters: 开始的聚类中心数量\n",
    "    - init 初始化方法 ，默认为‘k-means++’\n",
    " ***\n",
    "    - labels_默认标记的类型，可以用于与真实值比较\n",
    "    \n",
    "## 1.3 案例\n",
    "+ 对Instacart Market用户聚类\n",
    "\n",
    "## 1.4 性能评估指标\n",
    "### 1.4.1 单个点的轮廓系数\n",
    "$$\n",
    "sc_j = \\frac {b_i-a_i}{max(b_ia_i)} \\\\ b_i 外部距离，a_i 为内部距离\n",
    "$$\n",
    "   - 对每个点i为已聚类数据中的样本，$b_i$为i到其他族群（按族群计算平均距离）所有样本的距离的最小值，$a_i$为i到本身簇的距离平均值\n",
    "        - 最终计算出所有样本点的轮廓系数平均值\n",
    "    - 当$b_i$ >> $a_i$ :1最优\n",
    "    - 当$b_i$ >> $a_i$ :-1最差\n",
    "    - 一般轮廓系数超过0.1时，性能已经属于比较好的\n",
    "### 1.4.2 评估api\n",
    "+ sklearn.metrics.silhouette_score(X,labels)\n",
    "    - 计算轮廓系数\n",
    "    - X,特征值\n",
    "    - labels，被聚类标记的目标值\n",
    "    \n",
    "### 1.4.3 总结\n",
    "+ 类似迭代算法，逐步求得中心点\n",
    "+ 缺点：容易收敛到局部最优解（多次聚类），可以多次随机初始化进行优化\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "prior = pd.read_csv(r'./market_csv/order_products__prior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('./market_csv/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv(r'./market_csv/orders.csv')\n",
    "aisles = pd.read_csv('./market_csv/aisles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mg = pd.merge(prior,products,on=['product_id','product_id'])\n",
    "_mg = pd.merge(_mg,orders,on=['order_id','order_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = pd.merge(_mg,aisles,on=['aisle_id','aisle_id'])\n",
    "cross = pd.crosstab(me['user_id'],me['aisle'])\n",
    "pca = PCA(n_components=0.9)#保留百分之九十特征\n",
    "data = pca.fit_transform(cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206209, 27)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 27)\n"
     ]
    }
   ],
   "source": [
    "x = data[:500]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0\n",
      " 2 0 0 0 0 0 0 0 0 0 0 2 0 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 2 0 0\n",
      " 0 0 0 2 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 2 0 0 0 0 3 1 0 0 0 2 0 0 0 0 0 0 0 0\n",
      " 3 0 0 0 2 0 0 0 0 2 2 2 0 2 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 2 0 3 0 0 0 2 0 0 0 0 0 0\n",
      " 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 2 0 0 2 2 0 2 0 0 0 0 0 0 0 2 0 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 0 2 2 0 0 0 0 0 0 0 2 2 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=4,)\n",
    "km.fit(x)\n",
    "predict = km.predict(x)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聚类效果轮廓系数输出： 0.6232921375779652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print(\"聚类效果轮廓系数输出：\",silhouette_score(x,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10),dpi=50)\n",
    "colored = ['orange','green','blue','purple']\n",
    "colr = [colored[i] for i in predict]\n",
    "plt.scatter(x[:,1],x[:,20],color=colr)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218.854px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "459px",
    "left": "1255px",
    "right": "20px",
    "top": "143px",
    "width": "656px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
